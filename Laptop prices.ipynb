{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bf330ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a14e7948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(896, 23)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>model</th>\n",
       "      <th>processor_brand</th>\n",
       "      <th>processor_name</th>\n",
       "      <th>processor_gnrtn</th>\n",
       "      <th>ram_gb</th>\n",
       "      <th>ram_type</th>\n",
       "      <th>ssd</th>\n",
       "      <th>hdd</th>\n",
       "      <th>os</th>\n",
       "      <th>...</th>\n",
       "      <th>display_size</th>\n",
       "      <th>warranty</th>\n",
       "      <th>Touchscreen</th>\n",
       "      <th>msoffice</th>\n",
       "      <th>latest_price</th>\n",
       "      <th>old_price</th>\n",
       "      <th>discount</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>ratings</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lenovo</td>\n",
       "      <td>A6-9225</td>\n",
       "      <td>AMD</td>\n",
       "      <td>A6-9225 Processor</td>\n",
       "      <td>10th</td>\n",
       "      <td>4 GB GB</td>\n",
       "      <td>DDR4</td>\n",
       "      <td>0 GB</td>\n",
       "      <td>1024 GB</td>\n",
       "      <td>Windows</td>\n",
       "      <td>...</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>24990</td>\n",
       "      <td>32790</td>\n",
       "      <td>23</td>\n",
       "      <td>3.7</td>\n",
       "      <td>63</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lenovo</td>\n",
       "      <td>Ideapad</td>\n",
       "      <td>AMD</td>\n",
       "      <td>APU Dual</td>\n",
       "      <td>10th</td>\n",
       "      <td>4 GB GB</td>\n",
       "      <td>DDR4</td>\n",
       "      <td>0 GB</td>\n",
       "      <td>512 GB</td>\n",
       "      <td>Windows</td>\n",
       "      <td>...</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>19590</td>\n",
       "      <td>21325</td>\n",
       "      <td>8</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1894</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Avita</td>\n",
       "      <td>PURA</td>\n",
       "      <td>AMD</td>\n",
       "      <td>APU Dual</td>\n",
       "      <td>10th</td>\n",
       "      <td>4 GB GB</td>\n",
       "      <td>DDR4</td>\n",
       "      <td>128 GB</td>\n",
       "      <td>0 GB</td>\n",
       "      <td>Windows</td>\n",
       "      <td>...</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>19990</td>\n",
       "      <td>27990</td>\n",
       "      <td>28</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1153</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Avita</td>\n",
       "      <td>PURA</td>\n",
       "      <td>AMD</td>\n",
       "      <td>APU Dual</td>\n",
       "      <td>10th</td>\n",
       "      <td>4 GB GB</td>\n",
       "      <td>DDR4</td>\n",
       "      <td>128 GB</td>\n",
       "      <td>0 GB</td>\n",
       "      <td>Windows</td>\n",
       "      <td>...</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>21490</td>\n",
       "      <td>27990</td>\n",
       "      <td>23</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1153</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Avita</td>\n",
       "      <td>PURA</td>\n",
       "      <td>AMD</td>\n",
       "      <td>APU Dual</td>\n",
       "      <td>10th</td>\n",
       "      <td>4 GB GB</td>\n",
       "      <td>DDR4</td>\n",
       "      <td>256 GB</td>\n",
       "      <td>0 GB</td>\n",
       "      <td>Windows</td>\n",
       "      <td>...</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>24990</td>\n",
       "      <td>33490</td>\n",
       "      <td>25</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1657</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    brand    model processor_brand     processor_name processor_gnrtn  \\\n",
       "0  Lenovo  A6-9225             AMD  A6-9225 Processor            10th   \n",
       "1  Lenovo  Ideapad             AMD           APU Dual            10th   \n",
       "2   Avita     PURA             AMD           APU Dual            10th   \n",
       "3   Avita     PURA             AMD           APU Dual            10th   \n",
       "4   Avita     PURA             AMD           APU Dual            10th   \n",
       "\n",
       "    ram_gb ram_type     ssd      hdd       os  ... display_size  warranty  \\\n",
       "0  4 GB GB     DDR4    0 GB  1024 GB  Windows  ...      Missing         0   \n",
       "1  4 GB GB     DDR4    0 GB   512 GB  Windows  ...      Missing         0   \n",
       "2  4 GB GB     DDR4  128 GB     0 GB  Windows  ...      Missing         0   \n",
       "3  4 GB GB     DDR4  128 GB     0 GB  Windows  ...      Missing         0   \n",
       "4  4 GB GB     DDR4  256 GB     0 GB  Windows  ...      Missing         0   \n",
       "\n",
       "  Touchscreen msoffice  latest_price old_price discount  star_rating  ratings  \\\n",
       "0          No       No         24990     32790       23          3.7       63   \n",
       "1          No       No         19590     21325        8          3.6     1894   \n",
       "2          No       No         19990     27990       28          3.7     1153   \n",
       "3          No       No         21490     27990       23          3.7     1153   \n",
       "4          No       No         24990     33490       25          3.7     1657   \n",
       "\n",
       "   reviews  \n",
       "0       12  \n",
       "1      256  \n",
       "2      159  \n",
       "3      159  \n",
       "4      234  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./Cleaned_Laptop_data.csv\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5faafe3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['model', 'processor_gnrtn', 'display_size'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns[df.isin(['Missing']).any()]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7de187e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[\"model\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f1d7e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.loc[df['model'] == \"Missing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48841c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop([\"model\"], axis = 1, inplace=True)\n",
    "df[\"brand\"] = df[\"brand\"] + df[\"model\"]\n",
    "df.drop([\"model\"], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1dac2a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>processor_brand</th>\n",
       "      <th>processor_name</th>\n",
       "      <th>processor_gnrtn</th>\n",
       "      <th>ram_gb</th>\n",
       "      <th>ram_type</th>\n",
       "      <th>ssd</th>\n",
       "      <th>hdd</th>\n",
       "      <th>os</th>\n",
       "      <th>os_bit</th>\n",
       "      <th>...</th>\n",
       "      <th>display_size</th>\n",
       "      <th>warranty</th>\n",
       "      <th>Touchscreen</th>\n",
       "      <th>msoffice</th>\n",
       "      <th>latest_price</th>\n",
       "      <th>old_price</th>\n",
       "      <th>discount</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>ratings</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ASUSVivoBook</td>\n",
       "      <td>Intel</td>\n",
       "      <td>Celeron Dual</td>\n",
       "      <td>Missing</td>\n",
       "      <td>4 GB GB</td>\n",
       "      <td>DDR4</td>\n",
       "      <td>0 GB</td>\n",
       "      <td>512 GB</td>\n",
       "      <td>Windows</td>\n",
       "      <td>64-bit</td>\n",
       "      <td>...</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>26990</td>\n",
       "      <td>28990</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ASUSEeeBook</td>\n",
       "      <td>Intel</td>\n",
       "      <td>Celeron Dual</td>\n",
       "      <td>Missing</td>\n",
       "      <td>4 GB GB</td>\n",
       "      <td>DDR4</td>\n",
       "      <td>0 GB</td>\n",
       "      <td>512 GB</td>\n",
       "      <td>Windows</td>\n",
       "      <td>64-bit</td>\n",
       "      <td>...</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>22990</td>\n",
       "      <td>27990</td>\n",
       "      <td>17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ASUSEeeBook</td>\n",
       "      <td>Intel</td>\n",
       "      <td>Celeron Dual</td>\n",
       "      <td>Missing</td>\n",
       "      <td>4 GB GB</td>\n",
       "      <td>DDR4</td>\n",
       "      <td>0 GB</td>\n",
       "      <td>512 GB</td>\n",
       "      <td>Windows</td>\n",
       "      <td>32-bit</td>\n",
       "      <td>...</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>21990</td>\n",
       "      <td>25990</td>\n",
       "      <td>15</td>\n",
       "      <td>3.5</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>AvitaCosmos</td>\n",
       "      <td>Intel</td>\n",
       "      <td>Celeron Dual</td>\n",
       "      <td>Missing</td>\n",
       "      <td>4 GB GB</td>\n",
       "      <td>DDR4</td>\n",
       "      <td>0 GB</td>\n",
       "      <td>512 GB</td>\n",
       "      <td>Windows</td>\n",
       "      <td>64-bit</td>\n",
       "      <td>...</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>17490</td>\n",
       "      <td>23490</td>\n",
       "      <td>25</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1120</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ASUSEeeBook</td>\n",
       "      <td>Intel</td>\n",
       "      <td>Celeron Dual</td>\n",
       "      <td>Missing</td>\n",
       "      <td>4 GB GB</td>\n",
       "      <td>DDR4</td>\n",
       "      <td>0 GB</td>\n",
       "      <td>512 GB</td>\n",
       "      <td>Windows</td>\n",
       "      <td>64-bit</td>\n",
       "      <td>...</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>22990</td>\n",
       "      <td>27990</td>\n",
       "      <td>17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>ASUSROG</td>\n",
       "      <td>AMD</td>\n",
       "      <td>Ryzen 9</td>\n",
       "      <td>Missing</td>\n",
       "      <td>4 GB GB</td>\n",
       "      <td>DDR4</td>\n",
       "      <td>1024 GB</td>\n",
       "      <td>0 GB</td>\n",
       "      <td>Windows</td>\n",
       "      <td>64-bit</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>135990</td>\n",
       "      <td>172990</td>\n",
       "      <td>21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>ASUSROG</td>\n",
       "      <td>AMD</td>\n",
       "      <td>Ryzen 9</td>\n",
       "      <td>Missing</td>\n",
       "      <td>4 GB GB</td>\n",
       "      <td>DDR4</td>\n",
       "      <td>1024 GB</td>\n",
       "      <td>0 GB</td>\n",
       "      <td>Windows</td>\n",
       "      <td>64-bit</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>144990</td>\n",
       "      <td>194990</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>ASUSRyzen</td>\n",
       "      <td>AMD</td>\n",
       "      <td>Ryzen 9</td>\n",
       "      <td>Missing</td>\n",
       "      <td>4 GB GB</td>\n",
       "      <td>DDR4</td>\n",
       "      <td>1024 GB</td>\n",
       "      <td>0 GB</td>\n",
       "      <td>Windows</td>\n",
       "      <td>64-bit</td>\n",
       "      <td>...</td>\n",
       "      <td>Missing</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>149990</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>ASUSRyzen</td>\n",
       "      <td>AMD</td>\n",
       "      <td>Ryzen 9</td>\n",
       "      <td>Missing</td>\n",
       "      <td>4 GB GB</td>\n",
       "      <td>DDR4</td>\n",
       "      <td>1024 GB</td>\n",
       "      <td>0 GB</td>\n",
       "      <td>Windows</td>\n",
       "      <td>64-bit</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>142990</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>SAMSUNGGalaxy</td>\n",
       "      <td>Qualcomm</td>\n",
       "      <td>Snapdragon 7c</td>\n",
       "      <td>Missing</td>\n",
       "      <td>4 GB GB</td>\n",
       "      <td>LPDDR4X</td>\n",
       "      <td>0 GB</td>\n",
       "      <td>512 GB</td>\n",
       "      <td>Windows</td>\n",
       "      <td>32-bit</td>\n",
       "      <td>...</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>38990</td>\n",
       "      <td>47990</td>\n",
       "      <td>18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>239 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             brand processor_brand processor_name processor_gnrtn   ram_gb  \\\n",
       "14    ASUSVivoBook           Intel   Celeron Dual         Missing  4 GB GB   \n",
       "15     ASUSEeeBook           Intel   Celeron Dual         Missing  4 GB GB   \n",
       "16     ASUSEeeBook           Intel   Celeron Dual         Missing  4 GB GB   \n",
       "23     AvitaCosmos           Intel   Celeron Dual         Missing  4 GB GB   \n",
       "24     ASUSEeeBook           Intel   Celeron Dual         Missing  4 GB GB   \n",
       "..             ...             ...            ...             ...      ...   \n",
       "890        ASUSROG             AMD        Ryzen 9         Missing  4 GB GB   \n",
       "891        ASUSROG             AMD        Ryzen 9         Missing  4 GB GB   \n",
       "892      ASUSRyzen             AMD        Ryzen 9         Missing  4 GB GB   \n",
       "893      ASUSRyzen             AMD        Ryzen 9         Missing  4 GB GB   \n",
       "894  SAMSUNGGalaxy        Qualcomm  Snapdragon 7c         Missing  4 GB GB   \n",
       "\n",
       "    ram_type      ssd     hdd       os  os_bit  ...  display_size warranty  \\\n",
       "14      DDR4     0 GB  512 GB  Windows  64-bit  ...       Missing        0   \n",
       "15      DDR4     0 GB  512 GB  Windows  64-bit  ...       Missing        0   \n",
       "16      DDR4     0 GB  512 GB  Windows  32-bit  ...       Missing        0   \n",
       "23      DDR4     0 GB  512 GB  Windows  64-bit  ...       Missing        0   \n",
       "24      DDR4     0 GB  512 GB  Windows  64-bit  ...       Missing        0   \n",
       "..       ...      ...     ...      ...     ...  ...           ...      ...   \n",
       "890     DDR4  1024 GB    0 GB  Windows  64-bit  ...            16        1   \n",
       "891     DDR4  1024 GB    0 GB  Windows  64-bit  ...            16        1   \n",
       "892     DDR4  1024 GB    0 GB  Windows  64-bit  ...       Missing        1   \n",
       "893     DDR4  1024 GB    0 GB  Windows  64-bit  ...            16        1   \n",
       "894  LPDDR4X     0 GB  512 GB  Windows  32-bit  ...       Missing        0   \n",
       "\n",
       "    Touchscreen  msoffice latest_price old_price  discount  star_rating  \\\n",
       "14           No        No        26990     28990         6          0.0   \n",
       "15           No        No        22990     27990        17          0.0   \n",
       "16           No        No        21990     25990        15          3.5   \n",
       "23           No        No        17490     23490        25          3.6   \n",
       "24           No        No        22990     27990        17          0.0   \n",
       "..          ...       ...          ...       ...       ...          ...   \n",
       "890          No        No       135990    172990        21          0.0   \n",
       "891          No        No       144990    194990        25          0.0   \n",
       "892          No        No       149990         0         0          0.0   \n",
       "893          No        No       142990         0         0          0.0   \n",
       "894          No        No        38990     47990        18          0.0   \n",
       "\n",
       "     ratings  reviews  \n",
       "14         0        0  \n",
       "15         0        0  \n",
       "16        31        3  \n",
       "23      1120      196  \n",
       "24         0        0  \n",
       "..       ...      ...  \n",
       "890        0        0  \n",
       "891        0        0  \n",
       "892        0        0  \n",
       "893        0        0  \n",
       "894        0        0  \n",
       "\n",
       "[239 rows x 22 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['processor_gnrtn'] == \"Missing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1778132",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>processor_brand</th>\n",
       "      <th>processor_name</th>\n",
       "      <th>processor_gnrtn</th>\n",
       "      <th>ram_gb</th>\n",
       "      <th>ram_type</th>\n",
       "      <th>ssd</th>\n",
       "      <th>hdd</th>\n",
       "      <th>os</th>\n",
       "      <th>os_bit</th>\n",
       "      <th>...</th>\n",
       "      <th>display_size</th>\n",
       "      <th>warranty</th>\n",
       "      <th>Touchscreen</th>\n",
       "      <th>msoffice</th>\n",
       "      <th>latest_price</th>\n",
       "      <th>old_price</th>\n",
       "      <th>discount</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>ratings</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [brand, processor_brand, processor_name, processor_gnrtn, ram_gb, ram_type, ssd, hdd, os, os_bit, graphic_card_gb, weight, display_size, warranty, Touchscreen, msoffice, latest_price, old_price, discount, star_rating, ratings, reviews]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 22 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['processor_gnrtn'].replace(to_replace='Missing', value= '', inplace= True)\n",
    "df.loc[df['processor_gnrtn'] == \"Missing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd055f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"processor_brand\"] = df[\"processor_brand\"] + df[\"processor_name\"] + df['processor_gnrtn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bea37cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>processor_brand</th>\n",
       "      <th>ram_gb</th>\n",
       "      <th>ram_type</th>\n",
       "      <th>ssd</th>\n",
       "      <th>hdd</th>\n",
       "      <th>os</th>\n",
       "      <th>os_bit</th>\n",
       "      <th>graphic_card_gb</th>\n",
       "      <th>weight</th>\n",
       "      <th>display_size</th>\n",
       "      <th>warranty</th>\n",
       "      <th>Touchscreen</th>\n",
       "      <th>msoffice</th>\n",
       "      <th>latest_price</th>\n",
       "      <th>old_price</th>\n",
       "      <th>discount</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>ratings</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LenovoA6-9225</td>\n",
       "      <td>AMDA6-9225 Processor10th</td>\n",
       "      <td>4 GB GB</td>\n",
       "      <td>DDR4</td>\n",
       "      <td>0 GB</td>\n",
       "      <td>1024 GB</td>\n",
       "      <td>Windows</td>\n",
       "      <td>64-bit</td>\n",
       "      <td>0</td>\n",
       "      <td>ThinNlight</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>24990</td>\n",
       "      <td>32790</td>\n",
       "      <td>23</td>\n",
       "      <td>3.7</td>\n",
       "      <td>63</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LenovoIdeapad</td>\n",
       "      <td>AMDAPU Dual10th</td>\n",
       "      <td>4 GB GB</td>\n",
       "      <td>DDR4</td>\n",
       "      <td>0 GB</td>\n",
       "      <td>512 GB</td>\n",
       "      <td>Windows</td>\n",
       "      <td>64-bit</td>\n",
       "      <td>0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>19590</td>\n",
       "      <td>21325</td>\n",
       "      <td>8</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1894</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AvitaPURA</td>\n",
       "      <td>AMDAPU Dual10th</td>\n",
       "      <td>4 GB GB</td>\n",
       "      <td>DDR4</td>\n",
       "      <td>128 GB</td>\n",
       "      <td>0 GB</td>\n",
       "      <td>Windows</td>\n",
       "      <td>64-bit</td>\n",
       "      <td>0</td>\n",
       "      <td>ThinNlight</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>19990</td>\n",
       "      <td>27990</td>\n",
       "      <td>28</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1153</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AvitaPURA</td>\n",
       "      <td>AMDAPU Dual10th</td>\n",
       "      <td>4 GB GB</td>\n",
       "      <td>DDR4</td>\n",
       "      <td>128 GB</td>\n",
       "      <td>0 GB</td>\n",
       "      <td>Windows</td>\n",
       "      <td>64-bit</td>\n",
       "      <td>0</td>\n",
       "      <td>ThinNlight</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>21490</td>\n",
       "      <td>27990</td>\n",
       "      <td>23</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1153</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AvitaPURA</td>\n",
       "      <td>AMDAPU Dual10th</td>\n",
       "      <td>4 GB GB</td>\n",
       "      <td>DDR4</td>\n",
       "      <td>256 GB</td>\n",
       "      <td>0 GB</td>\n",
       "      <td>Windows</td>\n",
       "      <td>64-bit</td>\n",
       "      <td>0</td>\n",
       "      <td>ThinNlight</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>24990</td>\n",
       "      <td>33490</td>\n",
       "      <td>25</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1657</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           brand           processor_brand   ram_gb ram_type     ssd      hdd  \\\n",
       "0  LenovoA6-9225  AMDA6-9225 Processor10th  4 GB GB     DDR4    0 GB  1024 GB   \n",
       "1  LenovoIdeapad           AMDAPU Dual10th  4 GB GB     DDR4    0 GB   512 GB   \n",
       "2      AvitaPURA           AMDAPU Dual10th  4 GB GB     DDR4  128 GB     0 GB   \n",
       "3      AvitaPURA           AMDAPU Dual10th  4 GB GB     DDR4  128 GB     0 GB   \n",
       "4      AvitaPURA           AMDAPU Dual10th  4 GB GB     DDR4  256 GB     0 GB   \n",
       "\n",
       "        os  os_bit  graphic_card_gb      weight display_size  warranty  \\\n",
       "0  Windows  64-bit                0  ThinNlight      Missing         0   \n",
       "1  Windows  64-bit                0      Casual      Missing         0   \n",
       "2  Windows  64-bit                0  ThinNlight      Missing         0   \n",
       "3  Windows  64-bit                0  ThinNlight      Missing         0   \n",
       "4  Windows  64-bit                0  ThinNlight      Missing         0   \n",
       "\n",
       "  Touchscreen msoffice  latest_price  old_price  discount  star_rating  \\\n",
       "0          No       No         24990      32790        23          3.7   \n",
       "1          No       No         19590      21325         8          3.6   \n",
       "2          No       No         19990      27990        28          3.7   \n",
       "3          No       No         21490      27990        23          3.7   \n",
       "4          No       No         24990      33490        25          3.7   \n",
       "\n",
       "   ratings  reviews  \n",
       "0       63       12  \n",
       "1     1894      256  \n",
       "2     1153      159  \n",
       "3     1153      159  \n",
       "4     1657      234  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df[\"processor_brand\"].unique()))\n",
    "df.drop([\"processor_name\", \"processor_gnrtn\"], axis = 1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adb7d5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop([\"processor_gnrtn\"], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1dd1472b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>processor_brand</th>\n",
       "      <th>ram_gb</th>\n",
       "      <th>ram_type</th>\n",
       "      <th>ssd</th>\n",
       "      <th>hdd</th>\n",
       "      <th>os</th>\n",
       "      <th>os_bit</th>\n",
       "      <th>graphic_card_gb</th>\n",
       "      <th>weight</th>\n",
       "      <th>display_size</th>\n",
       "      <th>warranty</th>\n",
       "      <th>Touchscreen</th>\n",
       "      <th>msoffice</th>\n",
       "      <th>latest_price</th>\n",
       "      <th>old_price</th>\n",
       "      <th>discount</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>ratings</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LenovoA6-9225</td>\n",
       "      <td>AMDA6-9225 Processor10th</td>\n",
       "      <td>4 GB GB</td>\n",
       "      <td>DDR4</td>\n",
       "      <td>0 GB</td>\n",
       "      <td>1024 GB</td>\n",
       "      <td>Windows</td>\n",
       "      <td>64-bit</td>\n",
       "      <td>0</td>\n",
       "      <td>ThinNlight</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>24990</td>\n",
       "      <td>32790</td>\n",
       "      <td>23</td>\n",
       "      <td>3.7</td>\n",
       "      <td>63</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LenovoIdeapad</td>\n",
       "      <td>AMDAPU Dual10th</td>\n",
       "      <td>4 GB GB</td>\n",
       "      <td>DDR4</td>\n",
       "      <td>0 GB</td>\n",
       "      <td>512 GB</td>\n",
       "      <td>Windows</td>\n",
       "      <td>64-bit</td>\n",
       "      <td>0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>19590</td>\n",
       "      <td>21325</td>\n",
       "      <td>8</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1894</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AvitaPURA</td>\n",
       "      <td>AMDAPU Dual10th</td>\n",
       "      <td>4 GB GB</td>\n",
       "      <td>DDR4</td>\n",
       "      <td>128 GB</td>\n",
       "      <td>0 GB</td>\n",
       "      <td>Windows</td>\n",
       "      <td>64-bit</td>\n",
       "      <td>0</td>\n",
       "      <td>ThinNlight</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>19990</td>\n",
       "      <td>27990</td>\n",
       "      <td>28</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1153</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AvitaPURA</td>\n",
       "      <td>AMDAPU Dual10th</td>\n",
       "      <td>4 GB GB</td>\n",
       "      <td>DDR4</td>\n",
       "      <td>128 GB</td>\n",
       "      <td>0 GB</td>\n",
       "      <td>Windows</td>\n",
       "      <td>64-bit</td>\n",
       "      <td>0</td>\n",
       "      <td>ThinNlight</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>21490</td>\n",
       "      <td>27990</td>\n",
       "      <td>23</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1153</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AvitaPURA</td>\n",
       "      <td>AMDAPU Dual10th</td>\n",
       "      <td>4 GB GB</td>\n",
       "      <td>DDR4</td>\n",
       "      <td>256 GB</td>\n",
       "      <td>0 GB</td>\n",
       "      <td>Windows</td>\n",
       "      <td>64-bit</td>\n",
       "      <td>0</td>\n",
       "      <td>ThinNlight</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>24990</td>\n",
       "      <td>33490</td>\n",
       "      <td>25</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1657</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>ASUSROG</td>\n",
       "      <td>AMDRyzen 9</td>\n",
       "      <td>4 GB GB</td>\n",
       "      <td>LPDDR4X</td>\n",
       "      <td>1024 GB</td>\n",
       "      <td>0 GB</td>\n",
       "      <td>Windows</td>\n",
       "      <td>64-bit</td>\n",
       "      <td>4</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Missing</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>124990</td>\n",
       "      <td>194990</td>\n",
       "      <td>35</td>\n",
       "      <td>4.2</td>\n",
       "      <td>44</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>ASUSVivoBook</td>\n",
       "      <td>AMDRyzen 9</td>\n",
       "      <td>4 GB GB</td>\n",
       "      <td>DDR4</td>\n",
       "      <td>1024 GB</td>\n",
       "      <td>0 GB</td>\n",
       "      <td>Windows</td>\n",
       "      <td>64-bit</td>\n",
       "      <td>4</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>116990</td>\n",
       "      <td>149990</td>\n",
       "      <td>22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>ASUSROG</td>\n",
       "      <td>AMDRyzen 9</td>\n",
       "      <td>4 GB GB</td>\n",
       "      <td>LPDDR4X</td>\n",
       "      <td>1024 GB</td>\n",
       "      <td>0 GB</td>\n",
       "      <td>Windows</td>\n",
       "      <td>64-bit</td>\n",
       "      <td>4</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Missing</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>234990</td>\n",
       "      <td>350990</td>\n",
       "      <td>33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>ASUSRyzen</td>\n",
       "      <td>AMDRyzen 9</td>\n",
       "      <td>4 GB GB</td>\n",
       "      <td>DDR4</td>\n",
       "      <td>1024 GB</td>\n",
       "      <td>0 GB</td>\n",
       "      <td>Windows</td>\n",
       "      <td>64-bit</td>\n",
       "      <td>4</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Missing</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>149990</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>SAMSUNGGalaxy</td>\n",
       "      <td>QualcommSnapdragon 7c</td>\n",
       "      <td>4 GB GB</td>\n",
       "      <td>LPDDR4X</td>\n",
       "      <td>0 GB</td>\n",
       "      <td>512 GB</td>\n",
       "      <td>Windows</td>\n",
       "      <td>32-bit</td>\n",
       "      <td>0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Missing</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>38990</td>\n",
       "      <td>47990</td>\n",
       "      <td>18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>332 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             brand           processor_brand   ram_gb ram_type      ssd  \\\n",
       "0    LenovoA6-9225  AMDA6-9225 Processor10th  4 GB GB     DDR4     0 GB   \n",
       "1    LenovoIdeapad           AMDAPU Dual10th  4 GB GB     DDR4     0 GB   \n",
       "2        AvitaPURA           AMDAPU Dual10th  4 GB GB     DDR4   128 GB   \n",
       "3        AvitaPURA           AMDAPU Dual10th  4 GB GB     DDR4   128 GB   \n",
       "4        AvitaPURA           AMDAPU Dual10th  4 GB GB     DDR4   256 GB   \n",
       "..             ...                       ...      ...      ...      ...   \n",
       "883        ASUSROG                AMDRyzen 9  4 GB GB  LPDDR4X  1024 GB   \n",
       "887   ASUSVivoBook                AMDRyzen 9  4 GB GB     DDR4  1024 GB   \n",
       "889        ASUSROG                AMDRyzen 9  4 GB GB  LPDDR4X  1024 GB   \n",
       "892      ASUSRyzen                AMDRyzen 9  4 GB GB     DDR4  1024 GB   \n",
       "894  SAMSUNGGalaxy     QualcommSnapdragon 7c  4 GB GB  LPDDR4X     0 GB   \n",
       "\n",
       "         hdd       os  os_bit  graphic_card_gb      weight display_size  \\\n",
       "0    1024 GB  Windows  64-bit                0  ThinNlight      Missing   \n",
       "1     512 GB  Windows  64-bit                0      Casual      Missing   \n",
       "2       0 GB  Windows  64-bit                0  ThinNlight      Missing   \n",
       "3       0 GB  Windows  64-bit                0  ThinNlight      Missing   \n",
       "4       0 GB  Windows  64-bit                0  ThinNlight      Missing   \n",
       "..       ...      ...     ...              ...         ...          ...   \n",
       "883     0 GB  Windows  64-bit                4      Casual      Missing   \n",
       "887     0 GB  Windows  64-bit                4      Casual      Missing   \n",
       "889     0 GB  Windows  64-bit                4      Casual      Missing   \n",
       "892     0 GB  Windows  64-bit                4      Casual      Missing   \n",
       "894   512 GB  Windows  32-bit                0      Casual      Missing   \n",
       "\n",
       "     warranty Touchscreen msoffice  latest_price  old_price  discount  \\\n",
       "0           0          No       No         24990      32790        23   \n",
       "1           0          No       No         19590      21325         8   \n",
       "2           0          No       No         19990      27990        28   \n",
       "3           0          No       No         21490      27990        23   \n",
       "4           0          No       No         24990      33490        25   \n",
       "..        ...         ...      ...           ...        ...       ...   \n",
       "883         1         Yes      Yes        124990     194990        35   \n",
       "887         0          No       No        116990     149990        22   \n",
       "889         1         Yes      Yes        234990     350990        33   \n",
       "892         1          No       No        149990          0         0   \n",
       "894         0          No       No         38990      47990        18   \n",
       "\n",
       "     star_rating  ratings  reviews  \n",
       "0            3.7       63       12  \n",
       "1            3.6     1894      256  \n",
       "2            3.7     1153      159  \n",
       "3            3.7     1153      159  \n",
       "4            3.7     1657      234  \n",
       "..           ...      ...      ...  \n",
       "883          4.2       44       12  \n",
       "887          0.0        0        0  \n",
       "889          0.0        0        0  \n",
       "892          0.0        0        0  \n",
       "894          0.0        0        0  \n",
       "\n",
       "[332 rows x 20 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['display_size'] == \"Missing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c1ebe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"display_size\"], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1134520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Touchscreen'].replace(to_replace={'No':0, 'Yes':1}, inplace= True)\n",
    "len(df[\"Touchscreen\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "072fc085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['msoffice'].replace(to_replace={'No':0, 'Yes':1}, inplace= True)\n",
    "len(df[\"msoffice\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bf6c660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[\"ram_gb\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "435c7cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"ram_gb\"] = df[\"ram_gb\"].apply(lambda x : ''.join(filter(str.isdigit, x)))\n",
    "len(df[\"ram_gb\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0648ab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ssd\"] = df[\"ssd\"].apply(lambda x : ''.join(filter(str.isdigit, x)))\n",
    "df[\"hdd\"] = df[\"hdd\"].apply(lambda x : ''.join(filter(str.isdigit, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70632090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>processor_brand</th>\n",
       "      <th>ram_gb</th>\n",
       "      <th>ram_type</th>\n",
       "      <th>ssd</th>\n",
       "      <th>hdd</th>\n",
       "      <th>os</th>\n",
       "      <th>os_bit</th>\n",
       "      <th>graphic_card_gb</th>\n",
       "      <th>weight</th>\n",
       "      <th>warranty</th>\n",
       "      <th>Touchscreen</th>\n",
       "      <th>msoffice</th>\n",
       "      <th>latest_price</th>\n",
       "      <th>old_price</th>\n",
       "      <th>discount</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>ratings</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LenovoA6-9225</td>\n",
       "      <td>AMDA6-9225 Processor10th</td>\n",
       "      <td>4</td>\n",
       "      <td>DDR4</td>\n",
       "      <td>0</td>\n",
       "      <td>1024</td>\n",
       "      <td>Windows</td>\n",
       "      <td>64-bit</td>\n",
       "      <td>0</td>\n",
       "      <td>ThinNlight</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24990</td>\n",
       "      <td>32790</td>\n",
       "      <td>23</td>\n",
       "      <td>3.7</td>\n",
       "      <td>63</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LenovoIdeapad</td>\n",
       "      <td>AMDAPU Dual10th</td>\n",
       "      <td>4</td>\n",
       "      <td>DDR4</td>\n",
       "      <td>0</td>\n",
       "      <td>512</td>\n",
       "      <td>Windows</td>\n",
       "      <td>64-bit</td>\n",
       "      <td>0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19590</td>\n",
       "      <td>21325</td>\n",
       "      <td>8</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1894</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AvitaPURA</td>\n",
       "      <td>AMDAPU Dual10th</td>\n",
       "      <td>4</td>\n",
       "      <td>DDR4</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>Windows</td>\n",
       "      <td>64-bit</td>\n",
       "      <td>0</td>\n",
       "      <td>ThinNlight</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19990</td>\n",
       "      <td>27990</td>\n",
       "      <td>28</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1153</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AvitaPURA</td>\n",
       "      <td>AMDAPU Dual10th</td>\n",
       "      <td>4</td>\n",
       "      <td>DDR4</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>Windows</td>\n",
       "      <td>64-bit</td>\n",
       "      <td>0</td>\n",
       "      <td>ThinNlight</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21490</td>\n",
       "      <td>27990</td>\n",
       "      <td>23</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1153</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AvitaPURA</td>\n",
       "      <td>AMDAPU Dual10th</td>\n",
       "      <td>4</td>\n",
       "      <td>DDR4</td>\n",
       "      <td>256</td>\n",
       "      <td>0</td>\n",
       "      <td>Windows</td>\n",
       "      <td>64-bit</td>\n",
       "      <td>0</td>\n",
       "      <td>ThinNlight</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24990</td>\n",
       "      <td>33490</td>\n",
       "      <td>25</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1657</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           brand           processor_brand ram_gb ram_type  ssd   hdd  \\\n",
       "0  LenovoA6-9225  AMDA6-9225 Processor10th      4     DDR4    0  1024   \n",
       "1  LenovoIdeapad           AMDAPU Dual10th      4     DDR4    0   512   \n",
       "2      AvitaPURA           AMDAPU Dual10th      4     DDR4  128     0   \n",
       "3      AvitaPURA           AMDAPU Dual10th      4     DDR4  128     0   \n",
       "4      AvitaPURA           AMDAPU Dual10th      4     DDR4  256     0   \n",
       "\n",
       "        os  os_bit  graphic_card_gb      weight  warranty  Touchscreen  \\\n",
       "0  Windows  64-bit                0  ThinNlight         0            0   \n",
       "1  Windows  64-bit                0      Casual         0            0   \n",
       "2  Windows  64-bit                0  ThinNlight         0            0   \n",
       "3  Windows  64-bit                0  ThinNlight         0            0   \n",
       "4  Windows  64-bit                0  ThinNlight         0            0   \n",
       "\n",
       "   msoffice  latest_price  old_price  discount  star_rating  ratings  reviews  \n",
       "0         0         24990      32790        23          3.7       63       12  \n",
       "1         0         19590      21325         8          3.6     1894      256  \n",
       "2         0         19990      27990        28          3.7     1153      159  \n",
       "3         0         21490      27990        23          3.7     1153      159  \n",
       "4         0         24990      33490        25          3.7     1657      234  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6eae173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = df_2 = pd.get_dummies(df, columns=['ram_type', 'os', 'os_bit', 'brand', 'processor_brand', 'weight'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23c0f3d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ram_gb',\n",
       " 'ssd',\n",
       " 'hdd',\n",
       " 'graphic_card_gb',\n",
       " 'warranty',\n",
       " 'Touchscreen',\n",
       " 'msoffice',\n",
       " 'latest_price',\n",
       " 'old_price',\n",
       " 'discount',\n",
       " 'star_rating',\n",
       " 'ratings',\n",
       " 'reviews',\n",
       " 'ram_type_DDR4',\n",
       " 'ram_type_DDR5',\n",
       " 'ram_type_LPDDR3',\n",
       " 'ram_type_LPDDR4',\n",
       " 'ram_type_LPDDR4X',\n",
       " 'os_Mac',\n",
       " 'os_Windows',\n",
       " 'os_bit_64-bit',\n",
       " 'brand_APPLEMacBook',\n",
       " 'brand_ASUSAMD',\n",
       " 'brand_ASUSAPU',\n",
       " 'brand_ASUSASUS',\n",
       " 'brand_ASUSAsus',\n",
       " 'brand_ASUSCeleron',\n",
       " 'brand_ASUSChromebook',\n",
       " 'brand_ASUSCommercial',\n",
       " 'brand_ASUSEeeBook',\n",
       " 'brand_ASUSExpertBook',\n",
       " 'brand_ASUSF17',\n",
       " 'brand_ASUSIntel',\n",
       " 'brand_ASUSMissing',\n",
       " 'brand_ASUSPentium',\n",
       " 'brand_ASUSPro',\n",
       " 'brand_ASUSROG',\n",
       " 'brand_ASUSRog',\n",
       " 'brand_ASUSRyzen',\n",
       " 'brand_ASUSTUF',\n",
       " 'brand_ASUSVivo',\n",
       " 'brand_ASUSVivoBook',\n",
       " 'brand_ASUSVivoBook14',\n",
       " 'brand_ASUSZenBook',\n",
       " 'brand_ASUSZenbook',\n",
       " 'brand_ASUSZephyrus',\n",
       " 'brand_AvitaCosmos',\n",
       " 'brand_AvitaLiber',\n",
       " 'brand_AvitaPURA',\n",
       " 'brand_DELL3000',\n",
       " 'brand_DELL3511',\n",
       " 'brand_DELLDELL',\n",
       " 'brand_DELLG15',\n",
       " 'brand_DELLG3',\n",
       " 'brand_DELLG5',\n",
       " 'brand_DELLG7',\n",
       " 'brand_DELLGAMING',\n",
       " 'brand_DELLINSPIRON',\n",
       " 'brand_DELLInpiron',\n",
       " 'brand_DELLInspiron',\n",
       " 'brand_DELLInsprion',\n",
       " 'brand_DELLMissing',\n",
       " 'brand_DELLRyzen',\n",
       " 'brand_DELLVostro',\n",
       " 'brand_DELLXPS',\n",
       " 'brand_HP14a',\n",
       " 'brand_HP14s',\n",
       " 'brand_HP15',\n",
       " 'brand_HP15-ec1105AX',\n",
       " 'brand_HP15q',\n",
       " 'brand_HP15s',\n",
       " 'brand_HP250',\n",
       " 'brand_HP250-G6',\n",
       " 'brand_HP430',\n",
       " 'brand_HPAPU',\n",
       " 'brand_HPAthlon',\n",
       " 'brand_HPCeleron',\n",
       " 'brand_HPChromebook',\n",
       " 'brand_HPDA',\n",
       " 'brand_HPEnvy',\n",
       " 'brand_HPHP',\n",
       " 'brand_HPMissing',\n",
       " 'brand_HPNotebook',\n",
       " 'brand_HPOMEN',\n",
       " 'brand_HPOmen',\n",
       " 'brand_HPPavilion',\n",
       " 'brand_HPPentium',\n",
       " 'brand_HPRyzen',\n",
       " 'brand_HPSpectre',\n",
       " 'brand_HPx360',\n",
       " 'brand_InfinixINBook',\n",
       " 'brand_InfinixX1',\n",
       " 'brand_LGGram',\n",
       " 'brand_LenovoA6-9225',\n",
       " 'brand_LenovoAPU',\n",
       " 'brand_LenovoB50-70',\n",
       " 'brand_LenovoCeleron',\n",
       " 'brand_LenovoIDEAPAD',\n",
       " 'brand_LenovoIdeaPad',\n",
       " 'brand_LenovoIdeapad',\n",
       " 'brand_LenovoIntel',\n",
       " 'brand_LenovoLegion',\n",
       " 'brand_LenovoLenovo',\n",
       " 'brand_LenovoMissing',\n",
       " 'brand_LenovoRyzen',\n",
       " 'brand_LenovoThinkBook',\n",
       " 'brand_LenovoThinkPad',\n",
       " 'brand_LenovoThinkbook',\n",
       " 'brand_LenovoThinkpad',\n",
       " 'brand_LenovoThinpad',\n",
       " 'brand_LenovoV15',\n",
       " 'brand_LenovoX390',\n",
       " 'brand_LenovoYoga',\n",
       " 'brand_Lenovov15',\n",
       " 'brand_MICROSOFTSurface',\n",
       " 'brand_MSIAlpha',\n",
       " 'brand_MSIBravo',\n",
       " 'brand_MSICreator',\n",
       " 'brand_MSIDelta',\n",
       " 'brand_MSIGE76',\n",
       " 'brand_MSIGF63',\n",
       " 'brand_MSIGF65',\n",
       " 'brand_MSIGP65',\n",
       " 'brand_MSIGP76',\n",
       " 'brand_MSIGS',\n",
       " 'brand_MSIGS66',\n",
       " 'brand_MSIKatana',\n",
       " 'brand_MSIMissing',\n",
       " 'brand_MSIModern',\n",
       " 'brand_MSIPrestige',\n",
       " 'brand_MSIPulse',\n",
       " 'brand_MSIStealth',\n",
       " 'brand_MSISummit',\n",
       " 'brand_MSISword',\n",
       " 'brand_MSIWF65',\n",
       " 'brand_MiNotebook',\n",
       " 'brand_NokiaPureBook',\n",
       " 'brand_RedmiBook15',\n",
       " 'brand_RedmiBookPro',\n",
       " 'brand_SAMSUNGGalaxy',\n",
       " 'brand_Smartront.book',\n",
       " 'brand_VaioE',\n",
       " 'brand_VaioSE',\n",
       " 'brand_acerAspire',\n",
       " 'brand_acerChromebook',\n",
       " 'brand_acerConceptD',\n",
       " 'brand_acerDual',\n",
       " 'brand_acerExtensa',\n",
       " 'brand_acerIntel',\n",
       " 'brand_acerMissing',\n",
       " 'brand_acerNitro',\n",
       " 'brand_acerPredator',\n",
       " 'brand_acerSpin',\n",
       " 'brand_acerSwift',\n",
       " 'brand_acerTravelmate',\n",
       " 'brand_iballCompBook',\n",
       " 'brand_lenovoIdeapad',\n",
       " 'brand_lenovoYoga',\n",
       " 'brand_realmeBook',\n",
       " 'brand_realmeBook(Slim)',\n",
       " 'processor_brand_AMDAPU Dual10th',\n",
       " 'processor_brand_AMDAthlon Dual10th',\n",
       " 'processor_brand_AMDDual Core',\n",
       " 'processor_brand_AMDQuad10th',\n",
       " 'processor_brand_AMDRyzen 3',\n",
       " 'processor_brand_AMDRyzen 5',\n",
       " 'processor_brand_AMDRyzen 510th',\n",
       " 'processor_brand_AMDRyzen 7',\n",
       " 'processor_brand_AMDRyzen 9',\n",
       " 'processor_brand_AMDRyzen10th',\n",
       " 'processor_brand_IntelCeleron Dual',\n",
       " 'processor_brand_IntelCore',\n",
       " 'processor_brand_IntelCore i310th',\n",
       " 'processor_brand_IntelCore i311th',\n",
       " 'processor_brand_IntelCore i37th',\n",
       " 'processor_brand_IntelCore i38th',\n",
       " 'processor_brand_IntelCore i5',\n",
       " 'processor_brand_IntelCore i510th',\n",
       " 'processor_brand_IntelCore i511th',\n",
       " 'processor_brand_IntelCore i54th',\n",
       " 'processor_brand_IntelCore i57th',\n",
       " 'processor_brand_IntelCore i58th',\n",
       " 'processor_brand_IntelCore i59th',\n",
       " 'processor_brand_IntelCore i710th',\n",
       " 'processor_brand_IntelCore i711th',\n",
       " 'processor_brand_IntelCore i712th',\n",
       " 'processor_brand_IntelCore i78th',\n",
       " 'processor_brand_IntelCore i79th',\n",
       " 'processor_brand_IntelCore i910th',\n",
       " 'processor_brand_IntelCore i911th',\n",
       " 'processor_brand_IntelCore i912th',\n",
       " 'processor_brand_IntelCore m37th',\n",
       " 'processor_brand_IntelDual Core10th',\n",
       " 'processor_brand_IntelEver Screenpad10th',\n",
       " 'processor_brand_IntelGEFORCE RTX10th',\n",
       " 'processor_brand_IntelGeForce GTX10th',\n",
       " 'processor_brand_IntelGeForce RTX10th',\n",
       " 'processor_brand_IntelGenuine Windows10th',\n",
       " 'processor_brand_IntelHexa Core',\n",
       " 'processor_brand_IntelPentium Quad',\n",
       " 'processor_brand_IntelPentium Silver',\n",
       " 'processor_brand_IntelRyzen 7',\n",
       " 'processor_brand_M1M110th',\n",
       " 'processor_brand_MediaTekMediaTek Kompanio10th',\n",
       " 'processor_brand_QualcommSnapdragon 7c',\n",
       " 'weight_Gaming',\n",
       " 'weight_ThinNlight']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_2.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc50e79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(896, 207)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ram_gb</th>\n",
       "      <th>ssd</th>\n",
       "      <th>hdd</th>\n",
       "      <th>graphic_card_gb</th>\n",
       "      <th>warranty</th>\n",
       "      <th>Touchscreen</th>\n",
       "      <th>msoffice</th>\n",
       "      <th>latest_price</th>\n",
       "      <th>old_price</th>\n",
       "      <th>discount</th>\n",
       "      <th>...</th>\n",
       "      <th>processor_brand_IntelGenuine Windows10th</th>\n",
       "      <th>processor_brand_IntelHexa Core</th>\n",
       "      <th>processor_brand_IntelPentium Quad</th>\n",
       "      <th>processor_brand_IntelPentium Silver</th>\n",
       "      <th>processor_brand_IntelRyzen 7</th>\n",
       "      <th>processor_brand_M1M110th</th>\n",
       "      <th>processor_brand_MediaTekMediaTek Kompanio10th</th>\n",
       "      <th>processor_brand_QualcommSnapdragon 7c</th>\n",
       "      <th>weight_Gaming</th>\n",
       "      <th>weight_ThinNlight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24990.0</td>\n",
       "      <td>32790.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19590.0</td>\n",
       "      <td>21325.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19990.0</td>\n",
       "      <td>27990.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21490.0</td>\n",
       "      <td>27990.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24990.0</td>\n",
       "      <td>33490.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ram_gb    ssd     hdd  graphic_card_gb  warranty  Touchscreen  msoffice  \\\n",
       "0     4.0    0.0  1024.0              0.0       0.0          0.0       0.0   \n",
       "1     4.0    0.0   512.0              0.0       0.0          0.0       0.0   \n",
       "2     4.0  128.0     0.0              0.0       0.0          0.0       0.0   \n",
       "3     4.0  128.0     0.0              0.0       0.0          0.0       0.0   \n",
       "4     4.0  256.0     0.0              0.0       0.0          0.0       0.0   \n",
       "\n",
       "   latest_price  old_price  discount  ...  \\\n",
       "0       24990.0    32790.0      23.0  ...   \n",
       "1       19590.0    21325.0       8.0  ...   \n",
       "2       19990.0    27990.0      28.0  ...   \n",
       "3       21490.0    27990.0      23.0  ...   \n",
       "4       24990.0    33490.0      25.0  ...   \n",
       "\n",
       "   processor_brand_IntelGenuine Windows10th  processor_brand_IntelHexa Core  \\\n",
       "0                                       0.0                             0.0   \n",
       "1                                       0.0                             0.0   \n",
       "2                                       0.0                             0.0   \n",
       "3                                       0.0                             0.0   \n",
       "4                                       0.0                             0.0   \n",
       "\n",
       "   processor_brand_IntelPentium Quad  processor_brand_IntelPentium Silver  \\\n",
       "0                                0.0                                  0.0   \n",
       "1                                0.0                                  0.0   \n",
       "2                                0.0                                  0.0   \n",
       "3                                0.0                                  0.0   \n",
       "4                                0.0                                  0.0   \n",
       "\n",
       "   processor_brand_IntelRyzen 7  processor_brand_M1M110th  \\\n",
       "0                           0.0                       0.0   \n",
       "1                           0.0                       0.0   \n",
       "2                           0.0                       0.0   \n",
       "3                           0.0                       0.0   \n",
       "4                           0.0                       0.0   \n",
       "\n",
       "   processor_brand_MediaTekMediaTek Kompanio10th  \\\n",
       "0                                            0.0   \n",
       "1                                            0.0   \n",
       "2                                            0.0   \n",
       "3                                            0.0   \n",
       "4                                            0.0   \n",
       "\n",
       "   processor_brand_QualcommSnapdragon 7c  weight_Gaming  weight_ThinNlight  \n",
       "0                                    0.0            0.0                1.0  \n",
       "1                                    0.0            0.0                0.0  \n",
       "2                                    0.0            0.0                1.0  \n",
       "3                                    0.0            0.0                1.0  \n",
       "4                                    0.0            0.0                1.0  \n",
       "\n",
       "[5 rows x 207 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2 = df_2.astype(dtype=\"float64\")\n",
    "print(df_2.shape)\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6ae5065",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,df):\n",
    "        \n",
    "        df_x = df.drop([\"latest_price\"], axis = 1)\n",
    "        df_y = df[\"latest_price\"]\n",
    "        self.x=torch.tensor(df_x.values,dtype=torch.float32)\n",
    "        self.y=torch.tensor(df_y.values,dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.x[idx],self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6abfcf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e76db4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.33 * len(dataset))\n",
    "#train_size = int(0.5*len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3279503c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader =  torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader =  torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40763681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 206])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "data, labels = next(iter(train_loader))\n",
    "print(data.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ebb679fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 206])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "data, labels = next(iter(test_loader))\n",
    "print(data.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e454a525",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 206\n",
    "hidden_sizes = [int(input_size/4), 16]\n",
    "output_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3716a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=206, out_features=51, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=51, out_features=16, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "460c0b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e2a2a019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Epoch 1 - Training loss: 11938454.99661017, MAPE: 0.1550113707780838\n",
      "Starting epoch 2\n",
      "Epoch 2 - Training loss: 9531727.40338983, MAPE: 0.15542559325695038\n",
      "Starting epoch 3\n",
      "Epoch 3 - Training loss: 10427091.959322033, MAPE: 0.15546539425849915\n",
      "Starting epoch 4\n",
      "Epoch 4 - Training loss: 8935211.606779661, MAPE: 0.15553924441337585\n",
      "Starting epoch 5\n",
      "Epoch 5 - Training loss: 9116290.603389831, MAPE: 0.15565933287143707\n",
      "Starting epoch 6\n",
      "Epoch 6 - Training loss: 15347092.393220339, MAPE: 0.1558961421251297\n",
      "Starting epoch 7\n",
      "Epoch 7 - Training loss: 9308713.111864407, MAPE: 0.15659737586975098\n",
      "Starting epoch 8\n",
      "Epoch 8 - Training loss: 9363621.857627118, MAPE: 0.15699970722198486\n",
      "Starting epoch 9\n",
      "Epoch 9 - Training loss: 9235844.122033898, MAPE: 0.15694011747837067\n",
      "Starting epoch 10\n",
      "Epoch 10 - Training loss: 8870777.274576271, MAPE: 0.15672540664672852\n",
      "Starting epoch 11\n",
      "Epoch 11 - Training loss: 11558052.447457626, MAPE: 0.15657131373882294\n",
      "Starting epoch 12\n",
      "Epoch 12 - Training loss: 11766100.718644068, MAPE: 0.15612374246120453\n",
      "Starting epoch 13\n",
      "Epoch 13 - Training loss: 15082989.12542373, MAPE: 0.1553214192390442\n",
      "Starting epoch 14\n",
      "Epoch 14 - Training loss: 10100811.715254238, MAPE: 0.15434890985488892\n",
      "Starting epoch 15\n",
      "Epoch 15 - Training loss: 8777180.20338983, MAPE: 0.15360811352729797\n",
      "Starting epoch 16\n",
      "Epoch 16 - Training loss: 11731816.786440678, MAPE: 0.15300704538822174\n",
      "Starting epoch 17\n",
      "Epoch 17 - Training loss: 9890818.386440678, MAPE: 0.1524645835161209\n",
      "Starting epoch 18\n",
      "Epoch 18 - Training loss: 9380946.223728813, MAPE: 0.15206114947795868\n",
      "Starting epoch 19\n",
      "Epoch 19 - Training loss: 8621156.827118644, MAPE: 0.15184305608272552\n",
      "Starting epoch 20\n",
      "Epoch 20 - Training loss: 8609654.779661017, MAPE: 0.15191805362701416\n",
      "Starting epoch 21\n",
      "Epoch 21 - Training loss: 9406070.671186442, MAPE: 0.1522071212530136\n",
      "Starting epoch 22\n",
      "Epoch 22 - Training loss: 10800054.237288136, MAPE: 0.15219423174858093\n",
      "Starting epoch 23\n",
      "Epoch 23 - Training loss: 9325545.111864407, MAPE: 0.1519671529531479\n",
      "Starting epoch 24\n",
      "Epoch 24 - Training loss: 8341163.606779661, MAPE: 0.15199001133441925\n",
      "Starting epoch 25\n",
      "Epoch 25 - Training loss: 9823688.027118644, MAPE: 0.1522483080625534\n",
      "Starting epoch 26\n",
      "Epoch 26 - Training loss: 12113112.081355933, MAPE: 0.1526525914669037\n",
      "Starting epoch 27\n",
      "Epoch 27 - Training loss: 8765993.545762712, MAPE: 0.1529577672481537\n",
      "Starting epoch 28\n",
      "Epoch 28 - Training loss: 11061018.033898305, MAPE: 0.15307685732841492\n",
      "Starting epoch 29\n",
      "Epoch 29 - Training loss: 15676846.644067796, MAPE: 0.15323662757873535\n",
      "Starting epoch 30\n",
      "Epoch 30 - Training loss: 8439273.328813558, MAPE: 0.1532859355211258\n",
      "Starting epoch 31\n",
      "Epoch 31 - Training loss: 10905722.79322034, MAPE: 0.15359340608119965\n",
      "Starting epoch 32\n",
      "Epoch 32 - Training loss: 12821910.562711865, MAPE: 0.15394936501979828\n",
      "Starting epoch 33\n",
      "Epoch 33 - Training loss: 9065451.823728813, MAPE: 0.15381695330142975\n",
      "Starting epoch 34\n",
      "Epoch 34 - Training loss: 9131224.623728814, MAPE: 0.153569296002388\n",
      "Starting epoch 35\n",
      "Epoch 35 - Training loss: 14654335.674576271, MAPE: 0.15339767932891846\n",
      "Starting epoch 36\n",
      "Epoch 36 - Training loss: 8721812.61016949, MAPE: 0.15304052829742432\n",
      "Starting epoch 37\n",
      "Epoch 37 - Training loss: 13166252.691525424, MAPE: 0.15278349816799164\n",
      "Starting epoch 38\n",
      "Epoch 38 - Training loss: 12210344.244067797, MAPE: 0.15258607268333435\n",
      "Starting epoch 39\n",
      "Epoch 39 - Training loss: 10854897.030508475, MAPE: 0.15239016711711884\n",
      "Starting epoch 40\n",
      "Epoch 40 - Training loss: 10454678.345762711, MAPE: 0.15245474874973297\n",
      "Starting epoch 41\n",
      "Epoch 41 - Training loss: 8386700.040677967, MAPE: 0.15287630259990692\n",
      "Starting epoch 42\n",
      "Epoch 42 - Training loss: 9023003.010169491, MAPE: 0.15338638424873352\n",
      "Starting epoch 43\n",
      "Epoch 43 - Training loss: 17073668.98983051, MAPE: 0.15372143685817719\n",
      "Starting epoch 44\n",
      "Epoch 44 - Training loss: 12058434.169491526, MAPE: 0.1538482904434204\n",
      "Starting epoch 45\n",
      "Epoch 45 - Training loss: 9108138.413559321, MAPE: 0.15402103960514069\n",
      "Starting epoch 46\n",
      "Epoch 46 - Training loss: 8188255.186440678, MAPE: 0.15432079136371613\n",
      "Starting epoch 47\n",
      "Epoch 47 - Training loss: 9055168.433898306, MAPE: 0.15431833267211914\n",
      "Starting epoch 48\n",
      "Epoch 48 - Training loss: 9063934.26440678, MAPE: 0.1544569879770279\n",
      "Starting epoch 49\n",
      "Epoch 49 - Training loss: 9837799.918644067, MAPE: 0.15475133061408997\n",
      "Starting epoch 50\n",
      "Epoch 50 - Training loss: 11942359.647457628, MAPE: 0.15495069324970245\n",
      "Starting epoch 51\n",
      "Epoch 51 - Training loss: 9110380.149152542, MAPE: 0.15504278242588043\n",
      "Starting epoch 52\n",
      "Epoch 52 - Training loss: 8868969.437288135, MAPE: 0.15481235086917877\n",
      "Starting epoch 53\n",
      "Epoch 53 - Training loss: 9950921.328813558, MAPE: 0.1545753926038742\n",
      "Starting epoch 54\n",
      "Epoch 54 - Training loss: 11461540.013559323, MAPE: 0.15466438233852386\n",
      "Starting epoch 55\n",
      "Epoch 55 - Training loss: 8591229.722033897, MAPE: 0.154922217130661\n",
      "Starting epoch 56\n",
      "Epoch 56 - Training loss: 8647244.149152542, MAPE: 0.15481716394424438\n",
      "Starting epoch 57\n",
      "Epoch 57 - Training loss: 15971260.637288136, MAPE: 0.15464115142822266\n",
      "Starting epoch 58\n",
      "Epoch 58 - Training loss: 9056441.6, MAPE: 0.15420734882354736\n",
      "Starting epoch 59\n",
      "Epoch 59 - Training loss: 8859803.769491525, MAPE: 0.15400473773479462\n",
      "Starting epoch 60\n",
      "Epoch 60 - Training loss: 8748441.6, MAPE: 0.1540394276380539\n",
      "Starting epoch 61\n",
      "Epoch 61 - Training loss: 17458740.610169493, MAPE: 0.15380370616912842\n",
      "Starting epoch 62\n",
      "Epoch 62 - Training loss: 8279248.379661017, MAPE: 0.15358874201774597\n",
      "Starting epoch 63\n",
      "Epoch 63 - Training loss: 9369052.854237288, MAPE: 0.15336422622203827\n",
      "Starting epoch 64\n",
      "Epoch 64 - Training loss: 10970658.711864406, MAPE: 0.15345489978790283\n",
      "Starting epoch 65\n",
      "Epoch 65 - Training loss: 10084884.393220339, MAPE: 0.15367133915424347\n",
      "Starting epoch 66\n",
      "Epoch 66 - Training loss: 11408667.444067797, MAPE: 0.15354253351688385\n",
      "Starting epoch 67\n",
      "Epoch 67 - Training loss: 9074354.766101696, MAPE: 0.15305711328983307\n",
      "Starting epoch 68\n",
      "Epoch 68 - Training loss: 8510289.084745763, MAPE: 0.1527174562215805\n",
      "Starting epoch 69\n",
      "Epoch 69 - Training loss: 10809226.847457627, MAPE: 0.15240971744060516\n",
      "Starting epoch 70\n",
      "Epoch 70 - Training loss: 10915140.989830509, MAPE: 0.15220370888710022\n",
      "Starting epoch 71\n",
      "Epoch 71 - Training loss: 8407188.989830509, MAPE: 0.15221348404884338\n",
      "Starting epoch 72\n",
      "Epoch 72 - Training loss: 15140326.616949152, MAPE: 0.15207697451114655\n",
      "Starting epoch 73\n",
      "Epoch 73 - Training loss: 9567815.593220338, MAPE: 0.15197817981243134\n",
      "Starting epoch 74\n",
      "Epoch 74 - Training loss: 10277223.050847458, MAPE: 0.15235279500484467\n",
      "Starting epoch 75\n",
      "Epoch 75 - Training loss: 8546939.118644068, MAPE: 0.15300458669662476\n",
      "Starting epoch 76\n",
      "Epoch 76 - Training loss: 19981286.4, MAPE: 0.15369875729084015\n",
      "Starting epoch 77\n",
      "Epoch 77 - Training loss: 8438287.240677966, MAPE: 0.15457256138324738\n",
      "Starting epoch 78\n",
      "Epoch 78 - Training loss: 8305406.589830508, MAPE: 0.15525031089782715\n",
      "Starting epoch 79\n",
      "Epoch 79 - Training loss: 10820593.681355933, MAPE: 0.1552916318178177\n",
      "Starting epoch 80\n",
      "Epoch 80 - Training loss: 8167893.586440678, MAPE: 0.15481966733932495\n",
      "Starting epoch 81\n",
      "Epoch 81 - Training loss: 9644999.593220338, MAPE: 0.15399198234081268\n",
      "Starting epoch 82\n",
      "Epoch 82 - Training loss: 9013483.823728813, MAPE: 0.1528279036283493\n",
      "Starting epoch 83\n",
      "Epoch 83 - Training loss: 8895982.86101695, MAPE: 0.15212734043598175\n",
      "Starting epoch 84\n",
      "Epoch 84 - Training loss: 8688241.030508475, MAPE: 0.15182410180568695\n",
      "Starting epoch 85\n",
      "Epoch 85 - Training loss: 9664849.355932204, MAPE: 0.15160776674747467\n",
      "Starting epoch 86\n",
      "Epoch 86 - Training loss: 9821840.271186441, MAPE: 0.15159264206886292\n",
      "Starting epoch 87\n",
      "Epoch 87 - Training loss: 8730351.511864407, MAPE: 0.15177448093891144\n",
      "Starting epoch 88\n",
      "Epoch 88 - Training loss: 11048077.450847458, MAPE: 0.15210607647895813\n",
      "Starting epoch 89\n",
      "Epoch 89 - Training loss: 11398684.637288136, MAPE: 0.1522843986749649\n",
      "Starting epoch 90\n",
      "Epoch 90 - Training loss: 9399154.115254236, MAPE: 0.15280728042125702\n",
      "Starting epoch 91\n",
      "Epoch 91 - Training loss: 9591217.681355933, MAPE: 0.15309293568134308\n",
      "Starting epoch 92\n",
      "Epoch 92 - Training loss: 8401408.813559322, MAPE: 0.15303413569927216\n",
      "Starting epoch 93\n",
      "Epoch 93 - Training loss: 11982405.20677966, MAPE: 0.15292523801326752\n",
      "Starting epoch 94\n",
      "Epoch 94 - Training loss: 9857725.396610169, MAPE: 0.1530790627002716\n",
      "Starting epoch 95\n",
      "Epoch 95 - Training loss: 9809546.413559321, MAPE: 0.15328697860240936\n",
      "Starting epoch 96\n",
      "Epoch 96 - Training loss: 9116408.623728814, MAPE: 0.1534629762172699\n",
      "Starting epoch 97\n",
      "Epoch 97 - Training loss: 15203746.061016949, MAPE: 0.15354248881340027\n",
      "Starting epoch 98\n",
      "Epoch 98 - Training loss: 8874591.240677966, MAPE: 0.15349088609218597\n",
      "Starting epoch 99\n",
      "Epoch 99 - Training loss: 8441172.284745762, MAPE: 0.1535712033510208\n",
      "Starting epoch 100\n",
      "Epoch 100 - Training loss: 11895711.674576271, MAPE: 0.15368904173374176\n",
      "Starting epoch 101\n",
      "Epoch 101 - Training loss: 15165696.650847457, MAPE: 0.1538429707288742\n",
      "Starting epoch 102\n",
      "Epoch 102 - Training loss: 9436242.87457627, MAPE: 0.15421423316001892\n",
      "Starting epoch 103\n",
      "Epoch 103 - Training loss: 11312569.925423728, MAPE: 0.15440671145915985\n",
      "Starting epoch 104\n",
      "Epoch 104 - Training loss: 8636666.142372882, MAPE: 0.15430095791816711\n",
      "Starting epoch 105\n",
      "Epoch 105 - Training loss: 13788002.711864406, MAPE: 0.15421287715435028\n",
      "Starting epoch 106\n",
      "Epoch 106 - Training loss: 8935607.430508474, MAPE: 0.15444572269916534\n",
      "Starting epoch 107\n",
      "Epoch 107 - Training loss: 9678249.220338983, MAPE: 0.15492212772369385\n",
      "Starting epoch 108\n",
      "Epoch 108 - Training loss: 9741044.501694916, MAPE: 0.15548571944236755\n",
      "Starting epoch 109\n",
      "Epoch 109 - Training loss: 9280955.010169491, MAPE: 0.1556766778230667\n",
      "Starting epoch 110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110 - Training loss: 8605002.522033898, MAPE: 0.1555820256471634\n",
      "Starting epoch 111\n",
      "Epoch 111 - Training loss: 8442614.454237288, MAPE: 0.15538588166236877\n",
      "Starting epoch 112\n",
      "Epoch 112 - Training loss: 9790821.098305086, MAPE: 0.15489205718040466\n",
      "Starting epoch 113\n",
      "Epoch 113 - Training loss: 8169366.454237288, MAPE: 0.1541980654001236\n",
      "Starting epoch 114\n",
      "Epoch 114 - Training loss: 8081417.328813559, MAPE: 0.15351490676403046\n",
      "Starting epoch 115\n",
      "Epoch 115 - Training loss: 8221909.206779661, MAPE: 0.15277442336082458\n",
      "Starting epoch 116\n",
      "Epoch 116 - Training loss: 15150353.355932204, MAPE: 0.15196624398231506\n",
      "Starting epoch 117\n",
      "Epoch 117 - Training loss: 9011963.661016949, MAPE: 0.15147195756435394\n",
      "Starting epoch 118\n",
      "Epoch 118 - Training loss: 9280608.759322034, MAPE: 0.15137384831905365\n",
      "Starting epoch 119\n",
      "Epoch 119 - Training loss: 9915745.518644068, MAPE: 0.15124119818210602\n",
      "Starting epoch 120\n",
      "Epoch 120 - Training loss: 7938775.511864407, MAPE: 0.15126322209835052\n",
      "Starting epoch 121\n",
      "Epoch 121 - Training loss: 9058356.718644068, MAPE: 0.15158633887767792\n",
      "Starting epoch 122\n",
      "Epoch 122 - Training loss: 9903819.715254238, MAPE: 0.1519298553466797\n",
      "Starting epoch 123\n",
      "Epoch 123 - Training loss: 8314208.4338983055, MAPE: 0.15232022106647491\n",
      "Starting epoch 124\n",
      "Epoch 124 - Training loss: 13954005.044067796, MAPE: 0.1526813954114914\n",
      "Starting epoch 125\n",
      "Epoch 125 - Training loss: 8566470.94237288, MAPE: 0.15322116017341614\n",
      "Starting epoch 126\n",
      "Epoch 126 - Training loss: 8057745.030508474, MAPE: 0.1538839489221573\n",
      "Starting epoch 127\n",
      "Epoch 127 - Training loss: 8253729.030508474, MAPE: 0.1540239453315735\n",
      "Starting epoch 128\n",
      "Epoch 128 - Training loss: 8454112.325423729, MAPE: 0.15390166640281677\n",
      "Starting epoch 129\n",
      "Epoch 129 - Training loss: 8270236.962711864, MAPE: 0.1536867767572403\n",
      "Starting epoch 130\n",
      "Epoch 130 - Training loss: 8398445.884745764, MAPE: 0.15336494147777557\n",
      "Starting epoch 131\n",
      "Epoch 131 - Training loss: 12518807.430508474, MAPE: 0.15290148556232452\n",
      "Starting epoch 132\n",
      "Epoch 132 - Training loss: 9325059.037288135, MAPE: 0.1523570418357849\n",
      "Starting epoch 133\n",
      "Epoch 133 - Training loss: 8519152.054237287, MAPE: 0.15213139355182648\n",
      "Starting epoch 134\n",
      "Epoch 134 - Training loss: 8025975.2677966105, MAPE: 0.15217401087284088\n",
      "Starting epoch 135\n",
      "Epoch 135 - Training loss: 13609841.898305085, MAPE: 0.15219804644584656\n",
      "Starting epoch 136\n",
      "Epoch 136 - Training loss: 8307530.630508475, MAPE: 0.15234439074993134\n",
      "Starting epoch 137\n",
      "Epoch 137 - Training loss: 7917552.650847457, MAPE: 0.15223459899425507\n",
      "Starting epoch 138\n",
      "Epoch 138 - Training loss: 8234564.772881356, MAPE: 0.1518383026123047\n",
      "Starting epoch 139\n",
      "Epoch 139 - Training loss: 11840813.776271187, MAPE: 0.15157566964626312\n",
      "Starting epoch 140\n",
      "Epoch 140 - Training loss: 9890661.53220339, MAPE: 0.15140670537948608\n",
      "Starting epoch 141\n",
      "Epoch 141 - Training loss: 11856040.244067797, MAPE: 0.15128345787525177\n",
      "Starting epoch 142\n",
      "Epoch 142 - Training loss: 14639752.244067797, MAPE: 0.15170101821422577\n",
      "Starting epoch 143\n",
      "Epoch 143 - Training loss: 14781321.87118644, MAPE: 0.15234610438346863\n",
      "Starting epoch 144\n",
      "Epoch 144 - Training loss: 8486889.328813558, MAPE: 0.1528409868478775\n",
      "Starting epoch 145\n",
      "Epoch 145 - Training loss: 10283657.762711864, MAPE: 0.15283580124378204\n",
      "Starting epoch 146\n",
      "Epoch 146 - Training loss: 8197002.088135594, MAPE: 0.1526547372341156\n",
      "Starting epoch 147\n",
      "Epoch 147 - Training loss: 11132626.223728813, MAPE: 0.15251216292381287\n",
      "Starting epoch 148\n",
      "Epoch 148 - Training loss: 9233457.681355933, MAPE: 0.1524173468351364\n",
      "Starting epoch 149\n",
      "Epoch 149 - Training loss: 8563642.142372882, MAPE: 0.1526411771774292\n",
      "Starting epoch 150\n",
      "Epoch 150 - Training loss: 8563040.108474577, MAPE: 0.15298940241336823\n",
      "Starting epoch 151\n",
      "Epoch 151 - Training loss: 8596146.006779661, MAPE: 0.15321645140647888\n",
      "Starting epoch 152\n",
      "Epoch 152 - Training loss: 9632495.511864407, MAPE: 0.15329022705554962\n",
      "Starting epoch 153\n",
      "Epoch 153 - Training loss: 8239745.410169492, MAPE: 0.15312190353870392\n",
      "Starting epoch 154\n",
      "Epoch 154 - Training loss: 11293776.59661017, MAPE: 0.15282540023326874\n",
      "Starting epoch 155\n",
      "Epoch 155 - Training loss: 7974032.27118644, MAPE: 0.15263178944587708\n",
      "Starting epoch 156\n",
      "Epoch 156 - Training loss: 8423321.925423728, MAPE: 0.1525716334581375\n",
      "Starting epoch 157\n",
      "Epoch 157 - Training loss: 8314723.47118644, MAPE: 0.15252703428268433\n",
      "Starting epoch 158\n",
      "Epoch 158 - Training loss: 13959767.430508474, MAPE: 0.1526244580745697\n",
      "Starting epoch 159\n",
      "Epoch 159 - Training loss: 11736038.616949152, MAPE: 0.15305213630199432\n",
      "Starting epoch 160\n",
      "Epoch 160 - Training loss: 12714460.637288136, MAPE: 0.15335051715373993\n",
      "Starting epoch 161\n",
      "Epoch 161 - Training loss: 7905362.983050847, MAPE: 0.1532462239265442\n",
      "Starting epoch 162\n",
      "Epoch 162 - Training loss: 10810281.87118644, MAPE: 0.1528022140264511\n",
      "Starting epoch 163\n",
      "Epoch 163 - Training loss: 8425799.918644067, MAPE: 0.1523408442735672\n",
      "Starting epoch 164\n",
      "Epoch 164 - Training loss: 7988468.176271186, MAPE: 0.15205539762973785\n",
      "Starting epoch 165\n",
      "Epoch 165 - Training loss: 11993058.277966103, MAPE: 0.15183307230472565\n",
      "Starting epoch 166\n",
      "Epoch 166 - Training loss: 9692802.820338983, MAPE: 0.15166537463665009\n",
      "Starting epoch 167\n",
      "Epoch 167 - Training loss: 7714825.789830509, MAPE: 0.15186148881912231\n",
      "Starting epoch 168\n",
      "Epoch 168 - Training loss: 7842872.894915254, MAPE: 0.15199734270572662\n",
      "Starting epoch 169\n",
      "Epoch 169 - Training loss: 7793383.593220339, MAPE: 0.15181776881217957\n",
      "Starting epoch 170\n",
      "Epoch 170 - Training loss: 13075564.474576272, MAPE: 0.15147629380226135\n",
      "Starting epoch 171\n",
      "Epoch 171 - Training loss: 7583553.003389831, MAPE: 0.15128973126411438\n",
      "Starting epoch 172\n",
      "Epoch 172 - Training loss: 8461114.901694914, MAPE: 0.15143032371997833\n",
      "Starting epoch 173\n",
      "Epoch 173 - Training loss: 8212333.884745763, MAPE: 0.15173448622226715\n",
      "Starting epoch 174\n",
      "Epoch 174 - Training loss: 9237592.949152542, MAPE: 0.15233616530895233\n",
      "Starting epoch 175\n",
      "Epoch 175 - Training loss: 8780061.722033897, MAPE: 0.1527443826198578\n",
      "Starting epoch 176\n",
      "Epoch 176 - Training loss: 15567635.79661017, MAPE: 0.1532086879014969\n",
      "Starting epoch 177\n",
      "Epoch 177 - Training loss: 7682526.833898305, MAPE: 0.15365992486476898\n",
      "Starting epoch 178\n",
      "Epoch 178 - Training loss: 8669361.789830508, MAPE: 0.15366646647453308\n",
      "Starting epoch 179\n",
      "Epoch 179 - Training loss: 13530814.047457628, MAPE: 0.15321092307567596\n",
      "Starting epoch 180\n",
      "Epoch 180 - Training loss: 9180156.094915254, MAPE: 0.1528622955083847\n",
      "Starting epoch 181\n",
      "Epoch 181 - Training loss: 9671687.593220338, MAPE: 0.1525593101978302\n",
      "Starting epoch 182\n",
      "Epoch 182 - Training loss: 12054221.776271187, MAPE: 0.15235313773155212\n",
      "Starting epoch 183\n",
      "Epoch 183 - Training loss: 10658585.6, MAPE: 0.1522636115550995\n",
      "Starting epoch 184\n",
      "Epoch 184 - Training loss: 8840559.077966101, MAPE: 0.15198341012001038\n",
      "Starting epoch 185\n",
      "Epoch 185 - Training loss: 8767616.433898306, MAPE: 0.15174433588981628\n",
      "Starting epoch 186\n",
      "Epoch 186 - Training loss: 9020343.755932203, MAPE: 0.1516776978969574\n",
      "Starting epoch 187\n",
      "Epoch 187 - Training loss: 8455453.396610169, MAPE: 0.15180161595344543\n",
      "Starting epoch 188\n",
      "Epoch 188 - Training loss: 8741694.80677966, MAPE: 0.15186814963817596\n",
      "Starting epoch 189\n",
      "Epoch 189 - Training loss: 8734182.725423729, MAPE: 0.1518213301897049\n",
      "Starting epoch 190\n",
      "Epoch 190 - Training loss: 8962965.911864407, MAPE: 0.15160641074180603\n",
      "Starting epoch 191\n",
      "Epoch 191 - Training loss: 8204649.437288135, MAPE: 0.15119223296642303\n",
      "Starting epoch 192\n",
      "Epoch 192 - Training loss: 7477189.572881356, MAPE: 0.15081137418746948\n",
      "Starting epoch 193\n",
      "Epoch 193 - Training loss: 7952634.142372881, MAPE: 0.15056458115577698\n",
      "Starting epoch 194\n",
      "Epoch 194 - Training loss: 14487928.840677965, MAPE: 0.1502741426229477\n",
      "Starting epoch 195\n",
      "Epoch 195 - Training loss: 10754402.061016949, MAPE: 0.1502169817686081\n",
      "Starting epoch 196\n",
      "Epoch 196 - Training loss: 9225837.776271187, MAPE: 0.15038831532001495\n",
      "Starting epoch 197\n",
      "Epoch 197 - Training loss: 9354420.93559322, MAPE: 0.15048867464065552\n",
      "Starting epoch 198\n",
      "Epoch 198 - Training loss: 9018932.284745762, MAPE: 0.1509483903646469\n",
      "Starting epoch 199\n",
      "Epoch 199 - Training loss: 7934953.491525424, MAPE: 0.15151529014110565\n",
      "Starting epoch 200\n",
      "Epoch 200 - Training loss: 8636430.427118644, MAPE: 0.1516731232404709\n",
      "Starting epoch 201\n",
      "Epoch 201 - Training loss: 12669146.684745762, MAPE: 0.1518549621105194\n",
      "Starting epoch 202\n",
      "Epoch 202 - Training loss: 8282034.766101695, MAPE: 0.15228834748268127\n",
      "Starting epoch 203\n",
      "Epoch 203 - Training loss: 13186268.854237288, MAPE: 0.1525842249393463\n",
      "Starting epoch 204\n",
      "Epoch 204 - Training loss: 8568673.952542372, MAPE: 0.1528438776731491\n",
      "Starting epoch 205\n",
      "Epoch 205 - Training loss: 9631930.79322034, MAPE: 0.1529356688261032\n",
      "Starting epoch 206\n",
      "Epoch 206 - Training loss: 8203222.020338983, MAPE: 0.15267421305179596\n",
      "Starting epoch 207\n",
      "Epoch 207 - Training loss: 7794059.877966101, MAPE: 0.1520417034626007\n",
      "Starting epoch 208\n",
      "Epoch 208 - Training loss: 7584130.955932204, MAPE: 0.1512823849916458\n",
      "Starting epoch 209\n",
      "Epoch 209 - Training loss: 9421813.80338983, MAPE: 0.15067599713802338\n",
      "Starting epoch 210\n",
      "Epoch 210 - Training loss: 9267489.952542372, MAPE: 0.15031711757183075\n",
      "Starting epoch 211\n",
      "Epoch 211 - Training loss: 8909605.640677966, MAPE: 0.15005289018154144\n",
      "Starting epoch 212\n",
      "Epoch 212 - Training loss: 7860492.203389831, MAPE: 0.15013597905635834\n",
      "Starting epoch 213\n",
      "Epoch 213 - Training loss: 12661997.016949153, MAPE: 0.15030579268932343\n",
      "Starting epoch 214\n",
      "Epoch 214 - Training loss: 8052113.030508474, MAPE: 0.1506546586751938\n",
      "Starting epoch 215\n",
      "Epoch 215 - Training loss: 7937029.423728813, MAPE: 0.15126864612102509\n",
      "Starting epoch 216\n",
      "Epoch 216 - Training loss: 8747329.952542372, MAPE: 0.1517365425825119\n",
      "Starting epoch 217\n",
      "Epoch 217 - Training loss: 8062105.274576271, MAPE: 0.15201082825660706\n",
      "Starting epoch 218\n",
      "Epoch 218 - Training loss: 12623873.084745763, MAPE: 0.15225473046302795\n",
      "Starting epoch 219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 219 - Training loss: 7910541.450847457, MAPE: 0.15284806489944458\n",
      "Starting epoch 220\n",
      "Epoch 220 - Training loss: 11087981.776271187, MAPE: 0.153693288564682\n",
      "Starting epoch 221\n",
      "Epoch 221 - Training loss: 10621375.783050848, MAPE: 0.1543319970369339\n",
      "Starting epoch 222\n",
      "Epoch 222 - Training loss: 8023842.061016949, MAPE: 0.15433047711849213\n",
      "Starting epoch 223\n",
      "Epoch 223 - Training loss: 14360221.07118644, MAPE: 0.15360134840011597\n",
      "Starting epoch 224\n",
      "Epoch 224 - Training loss: 9820585.00338983, MAPE: 0.15262764692306519\n",
      "Starting epoch 225\n",
      "Epoch 225 - Training loss: 7797255.159322034, MAPE: 0.15209712088108063\n",
      "Starting epoch 226\n",
      "Epoch 226 - Training loss: 8301082.359322034, MAPE: 0.15200425684452057\n",
      "Starting epoch 227\n",
      "Epoch 227 - Training loss: 8252584.2440677965, MAPE: 0.15199044346809387\n",
      "Starting epoch 228\n",
      "Epoch 228 - Training loss: 7722138.46779661, MAPE: 0.15186786651611328\n",
      "Starting epoch 229\n",
      "Epoch 229 - Training loss: 10850173.830508474, MAPE: 0.15148361027240753\n",
      "Starting epoch 230\n",
      "Epoch 230 - Training loss: 7844166.2915254235, MAPE: 0.1506744921207428\n",
      "Starting epoch 231\n",
      "Epoch 231 - Training loss: 8618063.620338984, MAPE: 0.149933323264122\n",
      "Starting epoch 232\n",
      "Epoch 232 - Training loss: 8142455.430508475, MAPE: 0.1493184119462967\n",
      "Starting epoch 233\n",
      "Epoch 233 - Training loss: 9415840.325423729, MAPE: 0.1487375944852829\n",
      "Starting epoch 234\n",
      "Epoch 234 - Training loss: 9627432.786440678, MAPE: 0.14838291704654694\n",
      "Starting epoch 235\n",
      "Epoch 235 - Training loss: 10314043.444067797, MAPE: 0.14882296323776245\n",
      "Starting epoch 236\n",
      "Epoch 236 - Training loss: 8119890.440677966, MAPE: 0.14960597455501556\n",
      "Starting epoch 237\n",
      "Epoch 237 - Training loss: 13051109.640677966, MAPE: 0.15025261044502258\n",
      "Starting epoch 238\n",
      "Epoch 238 - Training loss: 7458340.149152542, MAPE: 0.15093263983726501\n",
      "Starting epoch 239\n",
      "Epoch 239 - Training loss: 8595651.905084746, MAPE: 0.1514052152633667\n",
      "Starting epoch 240\n",
      "Epoch 240 - Training loss: 8155667.416949153, MAPE: 0.15154987573623657\n",
      "Starting epoch 241\n",
      "Epoch 241 - Training loss: 8262979.037288136, MAPE: 0.15190507471561432\n",
      "Starting epoch 242\n",
      "Epoch 242 - Training loss: 8509835.93220339, MAPE: 0.1523730754852295\n",
      "Starting epoch 243\n",
      "Epoch 243 - Training loss: 9085559.53898305, MAPE: 0.15264488756656647\n",
      "Starting epoch 244\n",
      "Epoch 244 - Training loss: 9002064.488135593, MAPE: 0.15324106812477112\n",
      "Starting epoch 245\n",
      "Epoch 245 - Training loss: 8249567.783050847, MAPE: 0.15370330214500427\n",
      "Starting epoch 246\n",
      "Epoch 246 - Training loss: 13253312.216949152, MAPE: 0.15371984243392944\n",
      "Starting epoch 247\n",
      "Epoch 247 - Training loss: 7859607.322033898, MAPE: 0.15332600474357605\n",
      "Starting epoch 248\n",
      "Epoch 248 - Training loss: 7619252.122033899, MAPE: 0.15291161835193634\n",
      "Starting epoch 249\n",
      "Epoch 249 - Training loss: 9751430.94237288, MAPE: 0.15251336991786957\n",
      "Starting epoch 250\n",
      "Epoch 250 - Training loss: 12834641.13898305, MAPE: 0.15192675590515137\n",
      "Starting epoch 251\n",
      "Epoch 251 - Training loss: 9173251.688135594, MAPE: 0.15143336355686188\n",
      "Starting epoch 252\n",
      "Epoch 252 - Training loss: 7274812.420338983, MAPE: 0.15122905373573303\n",
      "Starting epoch 253\n",
      "Epoch 253 - Training loss: 9035719.810169492, MAPE: 0.15108312666416168\n",
      "Starting epoch 254\n",
      "Epoch 254 - Training loss: 7531585.030508474, MAPE: 0.15114112198352814\n",
      "Starting epoch 255\n",
      "Epoch 255 - Training loss: 11629115.877966102, MAPE: 0.1512630730867386\n",
      "Starting epoch 256\n",
      "Epoch 256 - Training loss: 8459039.674576271, MAPE: 0.15124325454235077\n",
      "Starting epoch 257\n",
      "Epoch 257 - Training loss: 7424433.355932203, MAPE: 0.1513565331697464\n",
      "Starting epoch 258\n",
      "Epoch 258 - Training loss: 8751094.020338982, MAPE: 0.15166312456130981\n",
      "Starting epoch 259\n",
      "Epoch 259 - Training loss: 9390250.522033898, MAPE: 0.15207119286060333\n",
      "Starting epoch 260\n",
      "Epoch 260 - Training loss: 10872590.969491525, MAPE: 0.15301735699176788\n",
      "Starting epoch 261\n",
      "Epoch 261 - Training loss: 11275261.830508474, MAPE: 0.15360897779464722\n",
      "Starting epoch 262\n",
      "Epoch 262 - Training loss: 9435730.440677967, MAPE: 0.15381492674350739\n",
      "Starting epoch 263\n",
      "Epoch 263 - Training loss: 8353673.979661017, MAPE: 0.15369170904159546\n",
      "Starting epoch 264\n",
      "Epoch 264 - Training loss: 7636568.189830508, MAPE: 0.153279647231102\n",
      "Starting epoch 265\n",
      "Epoch 265 - Training loss: 9573959.918644067, MAPE: 0.15270505845546722\n",
      "Starting epoch 266\n",
      "Epoch 266 - Training loss: 9559389.07118644, MAPE: 0.15207569301128387\n",
      "Starting epoch 267\n",
      "Epoch 267 - Training loss: 7374402.494915254, MAPE: 0.15137091279029846\n",
      "Starting epoch 268\n",
      "Epoch 268 - Training loss: 11711178.305084746, MAPE: 0.15084922313690186\n",
      "Starting epoch 269\n",
      "Epoch 269 - Training loss: 8693427.416949153, MAPE: 0.15051057934761047\n",
      "Starting epoch 270\n",
      "Epoch 270 - Training loss: 7688579.037288136, MAPE: 0.14998658001422882\n",
      "Starting epoch 271\n",
      "Epoch 271 - Training loss: 9423461.53220339, MAPE: 0.14948542416095734\n",
      "Starting epoch 272\n",
      "Epoch 272 - Training loss: 7522172.6372881355, MAPE: 0.1492641717195511\n",
      "Starting epoch 273\n",
      "Epoch 273 - Training loss: 15004059.769491525, MAPE: 0.14923082292079926\n",
      "Starting epoch 274\n",
      "Epoch 274 - Training loss: 9697444.122033898, MAPE: 0.14954116940498352\n",
      "Starting epoch 275\n",
      "Epoch 275 - Training loss: 9097495.213559322, MAPE: 0.1500210016965866\n",
      "Starting epoch 276\n",
      "Epoch 276 - Training loss: 9005372.311864406, MAPE: 0.15056632459163666\n",
      "Starting epoch 277\n",
      "Epoch 277 - Training loss: 12457627.986440677, MAPE: 0.15115228295326233\n",
      "Starting epoch 278\n",
      "Epoch 278 - Training loss: 12034300.962711865, MAPE: 0.15223179757595062\n",
      "Starting epoch 279\n",
      "Epoch 279 - Training loss: 10496367.511864407, MAPE: 0.15297387540340424\n",
      "Starting epoch 280\n",
      "Epoch 280 - Training loss: 12852615.593220338, MAPE: 0.15347792208194733\n",
      "Starting epoch 281\n",
      "Epoch 281 - Training loss: 9046302.589830508, MAPE: 0.15348713099956512\n",
      "Starting epoch 282\n",
      "Epoch 282 - Training loss: 8641994.73898305, MAPE: 0.153389111161232\n",
      "Starting epoch 283\n",
      "Epoch 283 - Training loss: 7530198.942372882, MAPE: 0.15339691936969757\n",
      "Starting epoch 284\n",
      "Epoch 284 - Training loss: 7761938.440677966, MAPE: 0.15305358171463013\n",
      "Starting epoch 285\n",
      "Epoch 285 - Training loss: 7077389.545762712, MAPE: 0.15242454409599304\n",
      "Starting epoch 286\n",
      "Epoch 286 - Training loss: 8048804.338983051, MAPE: 0.1518642157316208\n",
      "Starting epoch 287\n",
      "Epoch 287 - Training loss: 8431862.454237288, MAPE: 0.151350736618042\n",
      "Starting epoch 288\n",
      "Epoch 288 - Training loss: 10608178.657627119, MAPE: 0.1512521356344223\n",
      "Starting epoch 289\n",
      "Epoch 289 - Training loss: 10045087.132203389, MAPE: 0.15122345089912415\n",
      "Starting epoch 290\n",
      "Epoch 290 - Training loss: 8734606.969491525, MAPE: 0.15112198889255524\n",
      "Starting epoch 291\n",
      "Epoch 291 - Training loss: 7096185.681355932, MAPE: 0.15133024752140045\n",
      "Starting epoch 292\n",
      "Epoch 292 - Training loss: 11981389.12542373, MAPE: 0.1518438160419464\n",
      "Starting epoch 293\n",
      "Epoch 293 - Training loss: 8004029.938983051, MAPE: 0.15227249264717102\n",
      "Starting epoch 294\n",
      "Epoch 294 - Training loss: 8133313.73559322, MAPE: 0.1525793969631195\n",
      "Starting epoch 295\n",
      "Epoch 295 - Training loss: 8600711.593220338, MAPE: 0.15306051075458527\n",
      "Starting epoch 296\n",
      "Epoch 296 - Training loss: 10576031.240677966, MAPE: 0.15334376692771912\n",
      "Starting epoch 297\n",
      "Epoch 297 - Training loss: 12906274.603389831, MAPE: 0.15329726040363312\n",
      "Starting epoch 298\n",
      "Epoch 298 - Training loss: 7504373.098305085, MAPE: 0.15376245975494385\n",
      "Starting epoch 299\n",
      "Epoch 299 - Training loss: 7676278.562711865, MAPE: 0.15477964282035828\n",
      "Starting epoch 300\n",
      "Epoch 300 - Training loss: 8239448.298305085, MAPE: 0.15561959147453308\n",
      "Starting epoch 301\n",
      "Epoch 301 - Training loss: 8327054.752542373, MAPE: 0.15615615248680115\n",
      "Starting epoch 302\n",
      "Epoch 302 - Training loss: 8356272.813559322, MAPE: 0.15646959841251373\n",
      "Starting epoch 303\n",
      "Epoch 303 - Training loss: 11372001.19322034, MAPE: 0.1557885706424713\n",
      "Starting epoch 304\n",
      "Epoch 304 - Training loss: 8230056.352542372, MAPE: 0.15453563630580902\n",
      "Starting epoch 305\n",
      "Epoch 305 - Training loss: 7709607.810169492, MAPE: 0.15343081951141357\n",
      "Starting epoch 306\n",
      "Epoch 306 - Training loss: 7297461.152542373, MAPE: 0.15268874168395996\n",
      "Starting epoch 307\n",
      "Epoch 307 - Training loss: 7356792.894915254, MAPE: 0.15220977365970612\n",
      "Starting epoch 308\n",
      "Epoch 308 - Training loss: 9792463.40338983, MAPE: 0.1517372727394104\n",
      "Starting epoch 309\n",
      "Epoch 309 - Training loss: 9260367.132203389, MAPE: 0.15136127173900604\n",
      "Starting epoch 310\n",
      "Epoch 310 - Training loss: 7743678.372881356, MAPE: 0.15097592771053314\n",
      "Starting epoch 311\n",
      "Epoch 311 - Training loss: 8895012.447457626, MAPE: 0.15056876838207245\n",
      "Starting epoch 312\n",
      "Epoch 312 - Training loss: 7966149.423728813, MAPE: 0.15012423694133759\n",
      "Starting epoch 313\n",
      "Epoch 313 - Training loss: 8724097.73559322, MAPE: 0.15003211796283722\n",
      "Starting epoch 314\n",
      "Epoch 314 - Training loss: 7587649.844067797, MAPE: 0.1504964530467987\n",
      "Starting epoch 315\n",
      "Epoch 315 - Training loss: 7903927.213559322, MAPE: 0.15091927349567413\n",
      "Starting epoch 316\n",
      "Epoch 316 - Training loss: 8014894.101694915, MAPE: 0.1510157585144043\n",
      "Starting epoch 317\n",
      "Epoch 317 - Training loss: 7635453.505084746, MAPE: 0.1510995328426361\n",
      "Starting epoch 318\n",
      "Epoch 318 - Training loss: 8897138.766101696, MAPE: 0.1512049436569214\n",
      "Starting epoch 319\n",
      "Epoch 319 - Training loss: 8588116.393220339, MAPE: 0.15111136436462402\n",
      "Starting epoch 320\n",
      "Epoch 320 - Training loss: 12079429.20677966, MAPE: 0.1509520262479782\n",
      "Starting epoch 321\n",
      "Epoch 321 - Training loss: 11644378.250847457, MAPE: 0.15072371065616608\n",
      "Starting epoch 322\n",
      "Epoch 322 - Training loss: 7686334.155932203, MAPE: 0.15053094923496246\n",
      "Starting epoch 323\n",
      "Epoch 323 - Training loss: 8156026.359322034, MAPE: 0.150337353348732\n",
      "Starting epoch 324\n",
      "Epoch 324 - Training loss: 8518519.53898305, MAPE: 0.1498962640762329\n",
      "Starting epoch 325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 325 - Training loss: 7141513.871186441, MAPE: 0.1495850682258606\n",
      "Starting epoch 326\n",
      "Epoch 326 - Training loss: 7576719.0779661015, MAPE: 0.1494200974702835\n",
      "Starting epoch 327\n",
      "Epoch 327 - Training loss: 8069990.725423729, MAPE: 0.14924322068691254\n",
      "Starting epoch 328\n",
      "Epoch 328 - Training loss: 15448790.454237288, MAPE: 0.14946720004081726\n",
      "Starting epoch 329\n",
      "Epoch 329 - Training loss: 7244597.911864406, MAPE: 0.15041795372962952\n",
      "Starting epoch 330\n",
      "Epoch 330 - Training loss: 7308834.983050847, MAPE: 0.1515568047761917\n",
      "Starting epoch 331\n",
      "Epoch 331 - Training loss: 7006897.410169492, MAPE: 0.15208077430725098\n",
      "Starting epoch 332\n",
      "Epoch 332 - Training loss: 10357554.549152542, MAPE: 0.15219806134700775\n",
      "Starting epoch 333\n",
      "Epoch 333 - Training loss: 8241131.1728813555, MAPE: 0.15231414139270782\n",
      "Starting epoch 334\n",
      "Epoch 334 - Training loss: 7845159.593220339, MAPE: 0.15236172080039978\n",
      "Starting epoch 335\n",
      "Epoch 335 - Training loss: 7940494.969491526, MAPE: 0.1525946706533432\n",
      "Starting epoch 336\n",
      "Epoch 336 - Training loss: 8669219.905084746, MAPE: 0.15225890278816223\n",
      "Starting epoch 337\n",
      "Epoch 337 - Training loss: 7804917.477966102, MAPE: 0.1513984054327011\n",
      "Starting epoch 338\n",
      "Epoch 338 - Training loss: 9174803.091525424, MAPE: 0.1511772871017456\n",
      "Starting epoch 339\n",
      "Epoch 339 - Training loss: 8022325.911864406, MAPE: 0.15171371400356293\n",
      "Starting epoch 340\n",
      "Epoch 340 - Training loss: 6995527.40338983, MAPE: 0.15230585634708405\n",
      "Starting epoch 341\n",
      "Epoch 341 - Training loss: 7166569.057627118, MAPE: 0.15225750207901\n",
      "Starting epoch 342\n",
      "Epoch 342 - Training loss: 7342775.538983051, MAPE: 0.15162838995456696\n",
      "Starting epoch 343\n",
      "Epoch 343 - Training loss: 7574416.705084746, MAPE: 0.1511092483997345\n",
      "Starting epoch 344\n",
      "Epoch 344 - Training loss: 7293029.044067796, MAPE: 0.15098699927330017\n",
      "Starting epoch 345\n",
      "Epoch 345 - Training loss: 8741040.271186441, MAPE: 0.1509697586297989\n",
      "Starting epoch 346\n",
      "Epoch 346 - Training loss: 13992604.854237288, MAPE: 0.15124277770519257\n",
      "Starting epoch 347\n",
      "Epoch 347 - Training loss: 7219903.945762712, MAPE: 0.15165673196315765\n",
      "Starting epoch 348\n",
      "Epoch 348 - Training loss: 7227823.457627119, MAPE: 0.151563823223114\n",
      "Starting epoch 349\n",
      "Epoch 349 - Training loss: 7160482.657627119, MAPE: 0.15107901394367218\n",
      "Starting epoch 350\n",
      "Epoch 350 - Training loss: 7843240.2440677965, MAPE: 0.15065036714076996\n",
      "Starting epoch 351\n",
      "Epoch 351 - Training loss: 7249717.911864406, MAPE: 0.15034893155097961\n",
      "Starting epoch 352\n",
      "Epoch 352 - Training loss: 7019318.128813559, MAPE: 0.1499805897474289\n",
      "Starting epoch 353\n",
      "Epoch 353 - Training loss: 7012164.555932203, MAPE: 0.14969637989997864\n",
      "Starting epoch 354\n",
      "Epoch 354 - Training loss: 7214241.898305085, MAPE: 0.14955410361289978\n",
      "Starting epoch 355\n",
      "Epoch 355 - Training loss: 7509785.925423729, MAPE: 0.14946824312210083\n",
      "Starting epoch 356\n",
      "Epoch 356 - Training loss: 7924960.976271186, MAPE: 0.14956632256507874\n",
      "Starting epoch 357\n",
      "Epoch 357 - Training loss: 8475721.328813558, MAPE: 0.14968900382518768\n",
      "Starting epoch 358\n",
      "Epoch 358 - Training loss: 6887073.003389831, MAPE: 0.1496146023273468\n",
      "Starting epoch 359\n",
      "Epoch 359 - Training loss: 7077638.779661017, MAPE: 0.14946338534355164\n",
      "Starting epoch 360\n",
      "Epoch 360 - Training loss: 8150576.9220338985, MAPE: 0.1493397057056427\n",
      "Starting epoch 361\n",
      "Epoch 361 - Training loss: 7080859.552542373, MAPE: 0.14962835609912872\n",
      "Starting epoch 362\n",
      "Epoch 362 - Training loss: 8359608.840677966, MAPE: 0.1502506136894226\n",
      "Starting epoch 363\n",
      "Epoch 363 - Training loss: 7259233.844067797, MAPE: 0.15088960528373718\n",
      "Starting epoch 364\n",
      "Epoch 364 - Training loss: 10437616.59661017, MAPE: 0.15116164088249207\n",
      "Starting epoch 365\n",
      "Epoch 365 - Training loss: 7252491.715254237, MAPE: 0.15129801630973816\n",
      "Starting epoch 366\n",
      "Epoch 366 - Training loss: 7147915.823728814, MAPE: 0.1512073129415512\n",
      "Starting epoch 367\n",
      "Epoch 367 - Training loss: 7617146.250847458, MAPE: 0.1510133147239685\n",
      "Starting epoch 368\n",
      "Epoch 368 - Training loss: 10622869.911864407, MAPE: 0.15082389116287231\n",
      "Starting epoch 369\n",
      "Epoch 369 - Training loss: 9810188.366101695, MAPE: 0.15053260326385498\n",
      "Starting epoch 370\n",
      "Epoch 370 - Training loss: 7295861.694915255, MAPE: 0.15020990371704102\n",
      "Starting epoch 371\n",
      "Epoch 371 - Training loss: 9021801.654237289, MAPE: 0.14984193444252014\n",
      "Starting epoch 372\n",
      "Epoch 372 - Training loss: 8781897.545762712, MAPE: 0.1496461033821106\n",
      "Starting epoch 373\n",
      "Epoch 373 - Training loss: 7790204.094915254, MAPE: 0.14953681826591492\n",
      "Starting epoch 374\n",
      "Epoch 374 - Training loss: 7637979.444067797, MAPE: 0.14923837780952454\n",
      "Starting epoch 375\n",
      "Epoch 375 - Training loss: 10936837.315254238, MAPE: 0.14873705804347992\n",
      "Starting epoch 376\n",
      "Epoch 376 - Training loss: 7284359.593220339, MAPE: 0.14851120114326477\n",
      "Starting epoch 377\n",
      "Epoch 377 - Training loss: 8488554.73898305, MAPE: 0.14858867228031158\n",
      "Starting epoch 378\n",
      "Epoch 378 - Training loss: 9081754.250847457, MAPE: 0.14872033894062042\n",
      "Starting epoch 379\n",
      "Epoch 379 - Training loss: 8419063.972881356, MAPE: 0.1487714946269989\n",
      "Starting epoch 380\n",
      "Epoch 380 - Training loss: 8594151.484745763, MAPE: 0.14857037365436554\n",
      "Starting epoch 381\n",
      "Epoch 381 - Training loss: 11705516.908474576, MAPE: 0.14827781915664673\n",
      "Starting epoch 382\n",
      "Epoch 382 - Training loss: 7391277.450847457, MAPE: 0.14828595519065857\n",
      "Starting epoch 383\n",
      "Epoch 383 - Training loss: 7132611.579661017, MAPE: 0.1482117772102356\n",
      "Starting epoch 384\n",
      "Epoch 384 - Training loss: 7157788.745762712, MAPE: 0.14808917045593262\n",
      "Starting epoch 385\n",
      "Epoch 385 - Training loss: 7283167.023728814, MAPE: 0.1483381688594818\n",
      "Starting epoch 386\n",
      "Epoch 386 - Training loss: 8519225.708474576, MAPE: 0.14888839423656464\n",
      "Starting epoch 387\n",
      "Epoch 387 - Training loss: 8154342.833898305, MAPE: 0.14949004352092743\n",
      "Starting epoch 388\n",
      "Epoch 388 - Training loss: 6719463.511864407, MAPE: 0.15028998255729675\n",
      "Starting epoch 389\n",
      "Epoch 389 - Training loss: 11292616.46101695, MAPE: 0.15103329718112946\n",
      "Starting epoch 390\n",
      "Epoch 390 - Training loss: 8551263.45762712, MAPE: 0.15139047801494598\n",
      "Starting epoch 391\n",
      "Epoch 391 - Training loss: 9601260.8, MAPE: 0.15131589770317078\n",
      "Starting epoch 392\n",
      "Epoch 392 - Training loss: 7395788.583050847, MAPE: 0.1511237770318985\n",
      "Starting epoch 393\n",
      "Epoch 393 - Training loss: 8472658.006779661, MAPE: 0.15147319436073303\n",
      "Starting epoch 394\n",
      "Epoch 394 - Training loss: 8297276.962711864, MAPE: 0.1520216315984726\n",
      "Starting epoch 395\n",
      "Epoch 395 - Training loss: 7378272.325423729, MAPE: 0.15273752808570862\n",
      "Starting epoch 396\n",
      "Epoch 396 - Training loss: 10664679.484745763, MAPE: 0.15311302244663239\n",
      "Starting epoch 397\n",
      "Epoch 397 - Training loss: 11212983.647457628, MAPE: 0.1533452272415161\n",
      "Starting epoch 398\n",
      "Epoch 398 - Training loss: 11050429.830508474, MAPE: 0.15356481075286865\n",
      "Starting epoch 399\n",
      "Epoch 399 - Training loss: 11142131.416949153, MAPE: 0.1529802680015564\n",
      "Starting epoch 400\n",
      "Epoch 400 - Training loss: 7507819.06440678, MAPE: 0.15235121548175812\n",
      "Starting epoch 401\n",
      "Epoch 401 - Training loss: 8433910.888135593, MAPE: 0.15231885015964508\n",
      "Starting epoch 402\n",
      "Epoch 402 - Training loss: 7869953.627118644, MAPE: 0.15225480496883392\n",
      "Starting epoch 403\n",
      "Epoch 403 - Training loss: 9629157.315254238, MAPE: 0.1521749645471573\n",
      "Starting epoch 404\n",
      "Epoch 404 - Training loss: 6855642.359322034, MAPE: 0.1520615518093109\n",
      "Starting epoch 405\n",
      "Epoch 405 - Training loss: 8290161.898305085, MAPE: 0.15153466165065765\n",
      "Starting epoch 406\n",
      "Epoch 406 - Training loss: 7662461.071186441, MAPE: 0.1506420522928238\n",
      "Starting epoch 407\n",
      "Epoch 407 - Training loss: 7836383.891525424, MAPE: 0.14989612996578217\n",
      "Starting epoch 408\n",
      "Epoch 408 - Training loss: 7658738.006779661, MAPE: 0.14961744844913483\n",
      "Starting epoch 409\n",
      "Epoch 409 - Training loss: 9056736.433898306, MAPE: 0.14969389140605927\n",
      "Starting epoch 410\n",
      "Epoch 410 - Training loss: 11718484.176271187, MAPE: 0.14969639480113983\n",
      "Starting epoch 411\n",
      "Epoch 411 - Training loss: 7517865.871186441, MAPE: 0.1495424062013626\n",
      "Starting epoch 412\n",
      "Epoch 412 - Training loss: 10212557.342372881, MAPE: 0.1495419591665268\n",
      "Starting epoch 413\n",
      "Epoch 413 - Training loss: 7487687.918644068, MAPE: 0.14952655136585236\n",
      "Starting epoch 414\n",
      "Epoch 414 - Training loss: 7247758.752542373, MAPE: 0.14933860301971436\n",
      "Starting epoch 415\n",
      "Epoch 415 - Training loss: 8558339.47118644, MAPE: 0.1490633189678192\n",
      "Starting epoch 416\n",
      "Epoch 416 - Training loss: 6551837.613559322, MAPE: 0.14878371357917786\n",
      "Starting epoch 417\n",
      "Epoch 417 - Training loss: 7076949.477966102, MAPE: 0.14859764277935028\n",
      "Starting epoch 418\n",
      "Epoch 418 - Training loss: 9849175.213559322, MAPE: 0.14819934964179993\n",
      "Starting epoch 419\n",
      "Epoch 419 - Training loss: 7742205.179661017, MAPE: 0.14780032634735107\n",
      "Starting epoch 420\n",
      "Epoch 420 - Training loss: 6868147.3627118645, MAPE: 0.14759618043899536\n",
      "Starting epoch 421\n",
      "Epoch 421 - Training loss: 10775294.915254237, MAPE: 0.14738793671131134\n",
      "Starting epoch 422\n",
      "Epoch 422 - Training loss: 7020668.203389831, MAPE: 0.1475094109773636\n",
      "Starting epoch 423\n",
      "Epoch 423 - Training loss: 8643112.352542372, MAPE: 0.1479296237230301\n",
      "Starting epoch 424\n",
      "Epoch 424 - Training loss: 7354976.325423729, MAPE: 0.14815029501914978\n",
      "Starting epoch 425\n",
      "Epoch 425 - Training loss: 6983882.522033898, MAPE: 0.14834973216056824\n",
      "Starting epoch 426\n",
      "Epoch 426 - Training loss: 9849549.016949153, MAPE: 0.14853905141353607\n",
      "Starting epoch 427\n",
      "Epoch 427 - Training loss: 7907881.654237288, MAPE: 0.1485755294561386\n",
      "Starting epoch 428\n",
      "Epoch 428 - Training loss: 6822490.901694915, MAPE: 0.14870840311050415\n",
      "Starting epoch 429\n",
      "Epoch 429 - Training loss: 13121710.101694915, MAPE: 0.14882756769657135\n",
      "Starting epoch 430\n",
      "Epoch 430 - Training loss: 11249226.630508475, MAPE: 0.14907340705394745\n",
      "Starting epoch 431\n",
      "Epoch 431 - Training loss: 7849165.233898305, MAPE: 0.14930905401706696\n",
      "Starting epoch 432\n",
      "Epoch 432 - Training loss: 8065427.525423729, MAPE: 0.14955255389213562\n",
      "Starting epoch 433\n",
      "Epoch 433 - Training loss: 10532086.454237288, MAPE: 0.14969287812709808\n",
      "Starting epoch 434\n",
      "Epoch 434 - Training loss: 11230612.93559322, MAPE: 0.14951066672801971\n",
      "Starting epoch 435\n",
      "Epoch 435 - Training loss: 9706122.847457627, MAPE: 0.14970310032367706\n",
      "Starting epoch 436\n",
      "Epoch 436 - Training loss: 6500722.088135594, MAPE: 0.14992354810237885\n",
      "Starting epoch 437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 437 - Training loss: 7862466.169491526, MAPE: 0.14991095662117004\n",
      "Starting epoch 438\n",
      "Epoch 438 - Training loss: 6809964.203389831, MAPE: 0.1501094251871109\n",
      "Starting epoch 439\n",
      "Epoch 439 - Training loss: 10082112.325423729, MAPE: 0.1501200944185257\n",
      "Starting epoch 440\n",
      "Epoch 440 - Training loss: 7772938.522033898, MAPE: 0.14973153173923492\n",
      "Starting epoch 441\n",
      "Epoch 441 - Training loss: 6826263.430508475, MAPE: 0.14926719665527344\n",
      "Starting epoch 442\n",
      "Epoch 442 - Training loss: 10345029.20677966, MAPE: 0.14938820898532867\n",
      "Starting epoch 443\n",
      "Epoch 443 - Training loss: 6883229.39661017, MAPE: 0.14986371994018555\n",
      "Starting epoch 444\n",
      "Epoch 444 - Training loss: 7470742.996610169, MAPE: 0.1501922607421875\n",
      "Starting epoch 445\n",
      "Epoch 445 - Training loss: 7085326.101694915, MAPE: 0.1505303531885147\n",
      "Starting epoch 446\n",
      "Epoch 446 - Training loss: 10410782.589830508, MAPE: 0.15076805651187897\n",
      "Starting epoch 447\n",
      "Epoch 447 - Training loss: 11135040.867796611, MAPE: 0.1507912129163742\n",
      "Starting epoch 448\n",
      "Epoch 448 - Training loss: 7951393.410169492, MAPE: 0.15076886117458344\n",
      "Starting epoch 449\n",
      "Epoch 449 - Training loss: 7002247.701694915, MAPE: 0.1507115215063095\n",
      "Starting epoch 450\n",
      "Epoch 450 - Training loss: 7186621.071186441, MAPE: 0.15090163052082062\n",
      "Starting epoch 451\n",
      "Epoch 451 - Training loss: 7903297.518644067, MAPE: 0.15086713433265686\n",
      "Starting epoch 452\n",
      "Epoch 452 - Training loss: 11271260.149152542, MAPE: 0.15043267607688904\n",
      "Starting epoch 453\n",
      "Epoch 453 - Training loss: 7557061.857627119, MAPE: 0.15009117126464844\n",
      "Starting epoch 454\n",
      "Epoch 454 - Training loss: 7413490.8745762715, MAPE: 0.15029585361480713\n",
      "Starting epoch 455\n",
      "Epoch 455 - Training loss: 7967981.450847457, MAPE: 0.15034207701683044\n",
      "Starting epoch 456\n",
      "Epoch 456 - Training loss: 9519840.108474577, MAPE: 0.15012557804584503\n",
      "Starting epoch 457\n",
      "Epoch 457 - Training loss: 13098788.66440678, MAPE: 0.1498412787914276\n",
      "Starting epoch 458\n",
      "Epoch 458 - Training loss: 7312152.7322033895, MAPE: 0.14942000806331635\n",
      "Starting epoch 459\n",
      "Epoch 459 - Training loss: 7613670.833898305, MAPE: 0.14902296662330627\n",
      "Starting epoch 460\n",
      "Epoch 460 - Training loss: 7529338.006779661, MAPE: 0.14861710369586945\n",
      "Starting epoch 461\n",
      "Epoch 461 - Training loss: 6873283.47118644, MAPE: 0.1482885777950287\n",
      "Starting epoch 462\n",
      "Epoch 462 - Training loss: 6812100.555932203, MAPE: 0.14829689264297485\n",
      "Starting epoch 463\n",
      "Epoch 463 - Training loss: 7673718.779661017, MAPE: 0.1483219712972641\n",
      "Starting epoch 464\n",
      "Epoch 464 - Training loss: 9840118.345762711, MAPE: 0.14833354949951172\n",
      "Starting epoch 465\n",
      "Epoch 465 - Training loss: 7690026.955932204, MAPE: 0.14837700128555298\n",
      "Starting epoch 466\n",
      "Epoch 466 - Training loss: 9810016.108474577, MAPE: 0.14839544892311096\n",
      "Starting epoch 467\n",
      "Epoch 467 - Training loss: 10416380.094915254, MAPE: 0.14838509261608124\n",
      "Starting epoch 468\n",
      "Epoch 468 - Training loss: 7148526.969491526, MAPE: 0.14865738153457642\n",
      "Starting epoch 469\n",
      "Epoch 469 - Training loss: 6930917.098305085, MAPE: 0.14893145859241486\n",
      "Starting epoch 470\n",
      "Epoch 470 - Training loss: 7299959.864406779, MAPE: 0.14921289682388306\n",
      "Starting epoch 471\n",
      "Epoch 471 - Training loss: 6543904.542372881, MAPE: 0.1498674899339676\n",
      "Starting epoch 472\n",
      "Epoch 472 - Training loss: 7133649.030508474, MAPE: 0.15086670219898224\n",
      "Starting epoch 473\n",
      "Epoch 473 - Training loss: 6438321.73559322, MAPE: 0.15118806064128876\n",
      "Starting epoch 474\n",
      "Epoch 474 - Training loss: 9535228.854237288, MAPE: 0.1507641226053238\n",
      "Starting epoch 475\n",
      "Epoch 475 - Training loss: 7764289.73559322, MAPE: 0.15013587474822998\n",
      "Starting epoch 476\n",
      "Epoch 476 - Training loss: 6545165.993220339, MAPE: 0.1494426727294922\n",
      "Starting epoch 477\n",
      "Epoch 477 - Training loss: 10272589.66779661, MAPE: 0.14877744019031525\n",
      "Starting epoch 478\n",
      "Epoch 478 - Training loss: 7265712.705084746, MAPE: 0.14850211143493652\n",
      "Starting epoch 479\n",
      "Epoch 479 - Training loss: 6919742.481355933, MAPE: 0.14850522577762604\n",
      "Starting epoch 480\n",
      "Epoch 480 - Training loss: 9423353.708474576, MAPE: 0.1482987254858017\n",
      "Starting epoch 481\n",
      "Epoch 481 - Training loss: 6575937.247457627, MAPE: 0.14821745455265045\n",
      "Starting epoch 482\n",
      "Epoch 482 - Training loss: 10077148.420338983, MAPE: 0.14828893542289734\n",
      "Starting epoch 483\n",
      "Epoch 483 - Training loss: 10674824.352542372, MAPE: 0.14831556379795074\n",
      "Starting epoch 484\n",
      "Epoch 484 - Training loss: 6729048.135593221, MAPE: 0.14824827015399933\n",
      "Starting epoch 485\n",
      "Epoch 485 - Training loss: 6368578.6847457625, MAPE: 0.1483105570077896\n",
      "Starting epoch 486\n",
      "Epoch 486 - Training loss: 7686764.474576271, MAPE: 0.1486501395702362\n",
      "Starting epoch 487\n",
      "Epoch 487 - Training loss: 6729999.72881356, MAPE: 0.14917676150798798\n",
      "Starting epoch 488\n",
      "Epoch 488 - Training loss: 6779680.108474576, MAPE: 0.14976002275943756\n",
      "Starting epoch 489\n",
      "Epoch 489 - Training loss: 7552341.911864406, MAPE: 0.14999443292617798\n",
      "Starting epoch 490\n",
      "Epoch 490 - Training loss: 7052035.688135593, MAPE: 0.1500227451324463\n",
      "Starting epoch 491\n",
      "Epoch 491 - Training loss: 6346994.088135594, MAPE: 0.15023928880691528\n",
      "Starting epoch 492\n",
      "Epoch 492 - Training loss: 6497593.979661017, MAPE: 0.15038694441318512\n",
      "Starting epoch 493\n",
      "Epoch 493 - Training loss: 12340616.027118644, MAPE: 0.15018494427204132\n",
      "Starting epoch 494\n",
      "Epoch 494 - Training loss: 7244745.111864407, MAPE: 0.14979593455791473\n",
      "Starting epoch 495\n",
      "Epoch 495 - Training loss: 6380369.003389831, MAPE: 0.14948533475399017\n",
      "Starting epoch 496\n",
      "Epoch 496 - Training loss: 10761316.338983051, MAPE: 0.14914031326770782\n",
      "Starting epoch 497\n",
      "Epoch 497 - Training loss: 7588140.257627118, MAPE: 0.14885573089122772\n",
      "Starting epoch 498\n",
      "Epoch 498 - Training loss: 7162173.071186441, MAPE: 0.14896833896636963\n",
      "Starting epoch 499\n",
      "Epoch 499 - Training loss: 6966351.945762712, MAPE: 0.149361714720726\n",
      "Starting epoch 500\n",
      "Epoch 500 - Training loss: 10399394.277966103, MAPE: 0.1493777185678482\n",
      "Starting epoch 501\n",
      "Epoch 501 - Training loss: 8363296.976271186, MAPE: 0.1490636169910431\n",
      "Starting epoch 502\n",
      "Epoch 502 - Training loss: 7774317.776271187, MAPE: 0.1493886113166809\n",
      "Starting epoch 503\n",
      "Epoch 503 - Training loss: 6616864.108474576, MAPE: 0.15009507536888123\n",
      "Starting epoch 504\n",
      "Epoch 504 - Training loss: 7222285.66779661, MAPE: 0.15059423446655273\n",
      "Starting epoch 505\n",
      "Epoch 505 - Training loss: 7441850.901694915, MAPE: 0.15070943534374237\n",
      "Starting epoch 506\n",
      "Epoch 506 - Training loss: 8121563.118644068, MAPE: 0.1508129984140396\n",
      "Starting epoch 507\n",
      "Epoch 507 - Training loss: 6755784.2440677965, MAPE: 0.15098412334918976\n",
      "Starting epoch 508\n",
      "Epoch 508 - Training loss: 8938684.366101695, MAPE: 0.15100803971290588\n",
      "Starting epoch 509\n",
      "Epoch 509 - Training loss: 6400216.027118644, MAPE: 0.15071183443069458\n",
      "Starting epoch 510\n",
      "Epoch 510 - Training loss: 6926849.572881356, MAPE: 0.1499815434217453\n",
      "Starting epoch 511\n",
      "Epoch 511 - Training loss: 10083365.20677966, MAPE: 0.14923617243766785\n",
      "Starting epoch 512\n",
      "Epoch 512 - Training loss: 9136902.779661017, MAPE: 0.14896827936172485\n",
      "Starting epoch 513\n",
      "Epoch 513 - Training loss: 10271808.867796611, MAPE: 0.14938320219516754\n",
      "Starting epoch 514\n",
      "Epoch 514 - Training loss: 7352065.410169492, MAPE: 0.15001562237739563\n",
      "Starting epoch 515\n",
      "Epoch 515 - Training loss: 6641536.325423729, MAPE: 0.15040557086467743\n",
      "Starting epoch 516\n",
      "Epoch 516 - Training loss: 6773273.166101695, MAPE: 0.15006202459335327\n",
      "Starting epoch 517\n",
      "Epoch 517 - Training loss: 6469384.298305085, MAPE: 0.14924463629722595\n",
      "Starting epoch 518\n",
      "Epoch 518 - Training loss: 7419933.071186441, MAPE: 0.14865165948867798\n",
      "Starting epoch 519\n",
      "Epoch 519 - Training loss: 6831558.508474576, MAPE: 0.14831402897834778\n",
      "Starting epoch 520\n",
      "Epoch 520 - Training loss: 7276919.7559322035, MAPE: 0.1479777991771698\n",
      "Starting epoch 521\n",
      "Epoch 521 - Training loss: 6418827.986440678, MAPE: 0.14796403050422668\n",
      "Starting epoch 522\n",
      "Epoch 522 - Training loss: 7549064.677966102, MAPE: 0.1484489142894745\n",
      "Starting epoch 523\n",
      "Epoch 523 - Training loss: 9807434.847457627, MAPE: 0.1490495204925537\n",
      "Starting epoch 524\n",
      "Epoch 524 - Training loss: 9630622.155932203, MAPE: 0.149332195520401\n",
      "Starting epoch 525\n",
      "Epoch 525 - Training loss: 7049041.247457627, MAPE: 0.14918433129787445\n",
      "Starting epoch 526\n",
      "Epoch 526 - Training loss: 9162819.037288135, MAPE: 0.1489284187555313\n",
      "Starting epoch 527\n",
      "Epoch 527 - Training loss: 6796199.050847458, MAPE: 0.1488005816936493\n",
      "Starting epoch 528\n",
      "Epoch 528 - Training loss: 9519277.12542373, MAPE: 0.14874635636806488\n",
      "Starting epoch 529\n",
      "Epoch 529 - Training loss: 6748316.6372881355, MAPE: 0.14866624772548676\n",
      "Starting epoch 530\n",
      "Epoch 530 - Training loss: 7434354.33220339, MAPE: 0.14851941168308258\n",
      "Starting epoch 531\n",
      "Epoch 531 - Training loss: 6420448.4338983055, MAPE: 0.14865745604038239\n",
      "Starting epoch 532\n",
      "Epoch 532 - Training loss: 8714903.755932203, MAPE: 0.14894652366638184\n",
      "Starting epoch 533\n",
      "Epoch 533 - Training loss: 6612597.152542373, MAPE: 0.14900094270706177\n",
      "Starting epoch 534\n",
      "Epoch 534 - Training loss: 7815445.261016949, MAPE: 0.14892463386058807\n",
      "Starting epoch 535\n",
      "Epoch 535 - Training loss: 6566698.46779661, MAPE: 0.14887294173240662\n",
      "Starting epoch 536\n",
      "Epoch 536 - Training loss: 8078613.477966102, MAPE: 0.14890015125274658\n",
      "Starting epoch 537\n",
      "Epoch 537 - Training loss: 7446701.993220339, MAPE: 0.1491938680410385\n",
      "Starting epoch 538\n",
      "Epoch 538 - Training loss: 6638195.959322033, MAPE: 0.14938415586948395\n",
      "Starting epoch 539\n",
      "Epoch 539 - Training loss: 10565793.627118643, MAPE: 0.1496044546365738\n",
      "Starting epoch 540\n",
      "Epoch 540 - Training loss: 9512584.244067797, MAPE: 0.1504802405834198\n",
      "Starting epoch 541\n",
      "Epoch 541 - Training loss: 9339288.623728814, MAPE: 0.15105615556240082\n",
      "Starting epoch 542\n",
      "Epoch 542 - Training loss: 6524030.372881356, MAPE: 0.15098269283771515\n",
      "Starting epoch 543\n",
      "Epoch 543 - Training loss: 6334321.898305085, MAPE: 0.1508425623178482\n",
      "Starting epoch 544\n",
      "Epoch 544 - Training loss: 6591053.993220339, MAPE: 0.15087831020355225\n",
      "Starting epoch 545\n",
      "Epoch 545 - Training loss: 6447320.894915254, MAPE: 0.15083874762058258\n",
      "Starting epoch 546\n",
      "Epoch 546 - Training loss: 9726711.322033899, MAPE: 0.15087123215198517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 547\n",
      "Epoch 547 - Training loss: 6351932.203389831, MAPE: 0.15049892663955688\n",
      "Starting epoch 548\n",
      "Epoch 548 - Training loss: 6970351.0779661015, MAPE: 0.14963173866271973\n",
      "Starting epoch 549\n",
      "Epoch 549 - Training loss: 6647126.020338983, MAPE: 0.1488359421491623\n",
      "Starting epoch 550\n",
      "Epoch 550 - Training loss: 7694190.210169491, MAPE: 0.1480736881494522\n",
      "Starting epoch 551\n",
      "Epoch 551 - Training loss: 6298280.840677966, MAPE: 0.1474497765302658\n",
      "Starting epoch 552\n",
      "Epoch 552 - Training loss: 7255545.274576271, MAPE: 0.14704860746860504\n",
      "Starting epoch 553\n",
      "Epoch 553 - Training loss: 7837358.427118644, MAPE: 0.14702637493610382\n",
      "Starting epoch 554\n",
      "Epoch 554 - Training loss: 6563080.027118644, MAPE: 0.1470533013343811\n",
      "Starting epoch 555\n",
      "Epoch 555 - Training loss: 8102719.349152543, MAPE: 0.14707165956497192\n",
      "Starting epoch 556\n",
      "Epoch 556 - Training loss: 7186479.511864407, MAPE: 0.14706701040267944\n",
      "Starting epoch 557\n",
      "Epoch 557 - Training loss: 6900587.1728813555, MAPE: 0.1473236083984375\n",
      "Starting epoch 558\n",
      "Epoch 558 - Training loss: 9761435.444067797, MAPE: 0.14804811775684357\n",
      "Starting epoch 559\n",
      "Epoch 559 - Training loss: 6989903.837288136, MAPE: 0.14864638447761536\n",
      "Starting epoch 560\n",
      "Epoch 560 - Training loss: 7158036.393220339, MAPE: 0.14893412590026855\n",
      "Starting epoch 561\n",
      "Epoch 561 - Training loss: 6562445.559322034, MAPE: 0.1492854654788971\n",
      "Starting epoch 562\n",
      "Epoch 562 - Training loss: 7481050.142372881, MAPE: 0.14932650327682495\n",
      "Starting epoch 563\n",
      "Epoch 563 - Training loss: 9653533.938983051, MAPE: 0.14933563768863678\n",
      "Starting epoch 564\n",
      "Epoch 564 - Training loss: 6643490.386440678, MAPE: 0.14965073764324188\n",
      "Starting epoch 565\n",
      "Epoch 565 - Training loss: 7345878.562711865, MAPE: 0.1500210464000702\n",
      "Starting epoch 566\n",
      "Epoch 566 - Training loss: 9228881.572881356, MAPE: 0.15026752650737762\n",
      "Starting epoch 567\n",
      "Epoch 567 - Training loss: 6146964.013559322, MAPE: 0.15015862882137299\n",
      "Starting epoch 568\n",
      "Epoch 568 - Training loss: 6501845.369491525, MAPE: 0.1499827355146408\n",
      "Starting epoch 569\n",
      "Epoch 569 - Training loss: 6598661.966101695, MAPE: 0.1499832421541214\n",
      "Starting epoch 570\n",
      "Epoch 570 - Training loss: 6882237.288135593, MAPE: 0.14985570311546326\n",
      "Starting epoch 571\n",
      "Epoch 571 - Training loss: 6653919.457627119, MAPE: 0.14956019818782806\n",
      "Starting epoch 572\n",
      "Epoch 572 - Training loss: 7334553.491525424, MAPE: 0.14929692447185516\n",
      "Starting epoch 573\n",
      "Epoch 573 - Training loss: 6352311.050847458, MAPE: 0.14909647405147552\n",
      "Starting epoch 574\n",
      "Epoch 574 - Training loss: 6177891.525423729, MAPE: 0.14902998507022858\n",
      "Starting epoch 575\n",
      "Epoch 575 - Training loss: 6873941.477966102, MAPE: 0.14882522821426392\n",
      "Starting epoch 576\n",
      "Epoch 576 - Training loss: 6142301.830508474, MAPE: 0.1485142707824707\n",
      "Starting epoch 577\n",
      "Epoch 577 - Training loss: 10715698.983050847, MAPE: 0.1481991708278656\n",
      "Starting epoch 578\n",
      "Epoch 578 - Training loss: 6318915.742372882, MAPE: 0.14815418422222137\n",
      "Starting epoch 579\n",
      "Epoch 579 - Training loss: 6760210.115254237, MAPE: 0.14815932512283325\n",
      "Starting epoch 580\n",
      "Epoch 580 - Training loss: 6486045.613559322, MAPE: 0.1481306105852127\n",
      "Starting epoch 581\n",
      "Epoch 581 - Training loss: 6981361.572881356, MAPE: 0.14796550571918488\n",
      "Starting epoch 582\n",
      "Epoch 582 - Training loss: 6551562.088135594, MAPE: 0.14810797572135925\n",
      "Starting epoch 583\n",
      "Epoch 583 - Training loss: 8840051.742372882, MAPE: 0.14809489250183105\n",
      "Starting epoch 584\n",
      "Epoch 584 - Training loss: 7768572.854237288, MAPE: 0.14788652956485748\n",
      "Starting epoch 585\n",
      "Epoch 585 - Training loss: 9078039.755932203, MAPE: 0.14806413650512695\n",
      "Starting epoch 586\n",
      "Epoch 586 - Training loss: 6014674.847457627, MAPE: 0.14836353063583374\n",
      "Starting epoch 587\n",
      "Epoch 587 - Training loss: 10216701.613559322, MAPE: 0.14833058416843414\n",
      "Starting epoch 588\n",
      "Epoch 588 - Training loss: 9508569.491525425, MAPE: 0.14841490983963013\n",
      "Starting epoch 589\n",
      "Epoch 589 - Training loss: 6978929.898305085, MAPE: 0.14877085387706757\n",
      "Starting epoch 590\n",
      "Epoch 590 - Training loss: 6067591.457627119, MAPE: 0.14925868809223175\n",
      "Starting epoch 591\n",
      "Epoch 591 - Training loss: 6465825.084745763, MAPE: 0.14989018440246582\n",
      "Starting epoch 592\n",
      "Epoch 592 - Training loss: 6950960.59661017, MAPE: 0.15013185143470764\n",
      "Starting epoch 593\n",
      "Epoch 593 - Training loss: 7270778.576271187, MAPE: 0.14994193613529205\n",
      "Starting epoch 594\n",
      "Epoch 594 - Training loss: 6510504.027118644, MAPE: 0.1494649201631546\n",
      "Starting epoch 595\n",
      "Epoch 595 - Training loss: 6415986.440677966, MAPE: 0.1489986628293991\n",
      "Starting epoch 596\n",
      "Epoch 596 - Training loss: 6830333.722033898, MAPE: 0.14855195581912994\n",
      "Starting epoch 597\n",
      "Epoch 597 - Training loss: 6637306.793220339, MAPE: 0.1483360081911087\n",
      "Starting epoch 598\n",
      "Epoch 598 - Training loss: 6367286.020338983, MAPE: 0.1482963263988495\n",
      "Starting epoch 599\n",
      "Epoch 599 - Training loss: 6540415.13220339, MAPE: 0.14800666272640228\n",
      "Starting epoch 600\n",
      "Epoch 600 - Training loss: 6637811.525423729, MAPE: 0.14796479046344757\n",
      "Starting epoch 601\n",
      "Epoch 601 - Training loss: 5948674.250847458, MAPE: 0.14814522862434387\n",
      "Starting epoch 602\n",
      "Epoch 602 - Training loss: 6853304.298305085, MAPE: 0.1481131613254547\n",
      "Starting epoch 603\n",
      "Epoch 603 - Training loss: 9097723.444067797, MAPE: 0.14811351895332336\n",
      "Starting epoch 604\n",
      "Epoch 604 - Training loss: 7822646.996610169, MAPE: 0.1483689844608307\n",
      "Starting epoch 605\n",
      "Epoch 605 - Training loss: 6634836.284745763, MAPE: 0.14890453219413757\n",
      "Starting epoch 606\n",
      "Epoch 606 - Training loss: 6923058.006779661, MAPE: 0.14962078630924225\n",
      "Starting epoch 607\n",
      "Epoch 607 - Training loss: 6643563.06440678, MAPE: 0.15062129497528076\n",
      "Starting epoch 608\n",
      "Epoch 608 - Training loss: 6413808.9220338985, MAPE: 0.15120774507522583\n",
      "Starting epoch 609\n",
      "Epoch 609 - Training loss: 6602804.93559322, MAPE: 0.15092524886131287\n",
      "Starting epoch 610\n",
      "Epoch 610 - Training loss: 9785600.54237288, MAPE: 0.15014389157295227\n",
      "Starting epoch 611\n",
      "Epoch 611 - Training loss: 6311118.101694915, MAPE: 0.14940400421619415\n",
      "Starting epoch 612\n",
      "Epoch 612 - Training loss: 9649850.250847457, MAPE: 0.14880560338497162\n",
      "Starting epoch 613\n",
      "Epoch 613 - Training loss: 7066800.379661017, MAPE: 0.14834396541118622\n",
      "Starting epoch 614\n",
      "Epoch 614 - Training loss: 11456439.755932203, MAPE: 0.14803962409496307\n",
      "Starting epoch 615\n",
      "Epoch 615 - Training loss: 6527746.711864407, MAPE: 0.14796876907348633\n",
      "Starting epoch 616\n",
      "Epoch 616 - Training loss: 8684400.59661017, MAPE: 0.14794456958770752\n",
      "Starting epoch 617\n",
      "Epoch 617 - Training loss: 5920964.176271186, MAPE: 0.14788463711738586\n",
      "Starting epoch 618\n",
      "Epoch 618 - Training loss: 6661622.779661017, MAPE: 0.1476983278989792\n",
      "Starting epoch 619\n",
      "Epoch 619 - Training loss: 7153885.288135593, MAPE: 0.14758479595184326\n",
      "Starting epoch 620\n",
      "Epoch 620 - Training loss: 6223554.494915254, MAPE: 0.14728447794914246\n",
      "Starting epoch 621\n",
      "Epoch 621 - Training loss: 6320529.355932203, MAPE: 0.14703726768493652\n",
      "Starting epoch 622\n",
      "Epoch 622 - Training loss: 7114776.515254238, MAPE: 0.14712028205394745\n",
      "Starting epoch 623\n",
      "Epoch 623 - Training loss: 8604024.406779662, MAPE: 0.14725440740585327\n",
      "Starting epoch 624\n",
      "Epoch 624 - Training loss: 6680412.094915254, MAPE: 0.14716985821723938\n",
      "Starting epoch 625\n",
      "Epoch 625 - Training loss: 6221341.66779661, MAPE: 0.14713531732559204\n",
      "Starting epoch 626\n",
      "Epoch 626 - Training loss: 9454586.576271186, MAPE: 0.1471129059791565\n",
      "Starting epoch 627\n",
      "Epoch 627 - Training loss: 6396816.054237288, MAPE: 0.14731080830097198\n",
      "Starting epoch 628\n",
      "Epoch 628 - Training loss: 9469241.708474576, MAPE: 0.1480816751718521\n",
      "Starting epoch 629\n",
      "Epoch 629 - Training loss: 7121462.020338983, MAPE: 0.14888136088848114\n",
      "Starting epoch 630\n",
      "Epoch 630 - Training loss: 6575838.155932203, MAPE: 0.14962728321552277\n",
      "Starting epoch 631\n",
      "Epoch 631 - Training loss: 7587304.135593221, MAPE: 0.15021474659442902\n",
      "Starting epoch 632\n",
      "Epoch 632 - Training loss: 7116805.749152542, MAPE: 0.15021991729736328\n",
      "Starting epoch 633\n",
      "Epoch 633 - Training loss: 6107794.983050847, MAPE: 0.14980144798755646\n",
      "Starting epoch 634\n",
      "Epoch 634 - Training loss: 8588386.277966103, MAPE: 0.149286150932312\n",
      "Starting epoch 635\n",
      "Epoch 635 - Training loss: 7922175.349152543, MAPE: 0.1487492322921753\n",
      "Starting epoch 636\n",
      "Epoch 636 - Training loss: 6101117.613559322, MAPE: 0.14817827939987183\n",
      "Starting epoch 637\n",
      "Epoch 637 - Training loss: 7533639.593220339, MAPE: 0.14784155786037445\n",
      "Starting epoch 638\n",
      "Epoch 638 - Training loss: 9087437.016949153, MAPE: 0.14795146882534027\n",
      "Starting epoch 639\n",
      "Epoch 639 - Training loss: 6613403.227118644, MAPE: 0.14839045703411102\n",
      "Starting epoch 640\n",
      "Epoch 640 - Training loss: 9227756.040677967, MAPE: 0.1488097757101059\n",
      "Starting epoch 641\n",
      "Epoch 641 - Training loss: 6867611.444067797, MAPE: 0.14906585216522217\n",
      "Starting epoch 642\n",
      "Epoch 642 - Training loss: 6009603.037288136, MAPE: 0.14945469796657562\n",
      "Starting epoch 643\n",
      "Epoch 643 - Training loss: 6137226.793220339, MAPE: 0.14969275891780853\n",
      "Starting epoch 644\n",
      "Epoch 644 - Training loss: 8981083.118644068, MAPE: 0.14940395951271057\n",
      "Starting epoch 645\n",
      "Epoch 645 - Training loss: 5715814.915254237, MAPE: 0.14924518764019012\n",
      "Starting epoch 646\n",
      "Epoch 646 - Training loss: 6246807.864406779, MAPE: 0.1493404656648636\n",
      "Starting epoch 647\n",
      "Epoch 647 - Training loss: 5844089.301694916, MAPE: 0.1490393877029419\n",
      "Starting epoch 648\n",
      "Epoch 648 - Training loss: 7104769.193220339, MAPE: 0.14854201674461365\n",
      "Starting epoch 649\n",
      "Epoch 649 - Training loss: 8626448.488135593, MAPE: 0.14793598651885986\n",
      "Starting epoch 650\n",
      "Epoch 650 - Training loss: 6476038.725423729, MAPE: 0.14728572964668274\n",
      "Starting epoch 651\n",
      "Epoch 651 - Training loss: 5949868.149152542, MAPE: 0.14681251347064972\n",
      "Starting epoch 652\n",
      "Epoch 652 - Training loss: 8338692.772881356, MAPE: 0.146298348903656\n",
      "Starting epoch 653\n",
      "Epoch 653 - Training loss: 6076051.091525423, MAPE: 0.1458430290222168\n",
      "Starting epoch 654\n",
      "Epoch 654 - Training loss: 8667408.162711864, MAPE: 0.14556418359279633\n",
      "Starting epoch 655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 655 - Training loss: 10009367.105084745, MAPE: 0.14554248750209808\n",
      "Starting epoch 656\n",
      "Epoch 656 - Training loss: 8800448.0, MAPE: 0.14580081403255463\n",
      "Starting epoch 657\n",
      "Epoch 657 - Training loss: 6058366.752542373, MAPE: 0.14609244465827942\n",
      "Starting epoch 658\n",
      "Epoch 658 - Training loss: 5950572.040677967, MAPE: 0.1463712602853775\n",
      "Starting epoch 659\n",
      "Epoch 659 - Training loss: 6052627.3627118645, MAPE: 0.14644795656204224\n",
      "Starting epoch 660\n",
      "Epoch 660 - Training loss: 6548558.318644068, MAPE: 0.14673008024692535\n",
      "Starting epoch 661\n",
      "Epoch 661 - Training loss: 9513327.077966101, MAPE: 0.14751803874969482\n",
      "Starting epoch 662\n",
      "Epoch 662 - Training loss: 7489809.355932203, MAPE: 0.1487526148557663\n",
      "Starting epoch 663\n",
      "Epoch 663 - Training loss: 6224428.691525424, MAPE: 0.14973200857639313\n",
      "Starting epoch 664\n",
      "Epoch 664 - Training loss: 6533131.06440678, MAPE: 0.14988790452480316\n",
      "Starting epoch 665\n",
      "Epoch 665 - Training loss: 8753038.86101695, MAPE: 0.150064155459404\n",
      "Starting epoch 666\n",
      "Epoch 666 - Training loss: 6693185.73559322, MAPE: 0.1503690928220749\n",
      "Starting epoch 667\n",
      "Epoch 667 - Training loss: 6009179.715254237, MAPE: 0.1502775251865387\n",
      "Starting epoch 668\n",
      "Epoch 668 - Training loss: 5970094.427118644, MAPE: 0.14991465210914612\n",
      "Starting epoch 669\n",
      "Epoch 669 - Training loss: 8297307.118644068, MAPE: 0.14957189559936523\n",
      "Starting epoch 670\n",
      "Epoch 670 - Training loss: 6757165.776271187, MAPE: 0.14931708574295044\n",
      "Starting epoch 671\n",
      "Epoch 671 - Training loss: 8784464.705084745, MAPE: 0.14913280308246613\n",
      "Starting epoch 672\n",
      "Epoch 672 - Training loss: 6278344.027118644, MAPE: 0.14883849024772644\n",
      "Starting epoch 673\n",
      "Epoch 673 - Training loss: 7426180.555932203, MAPE: 0.14827203750610352\n",
      "Starting epoch 674\n",
      "Epoch 674 - Training loss: 6403151.945762712, MAPE: 0.14743375778198242\n",
      "Starting epoch 675\n",
      "Epoch 675 - Training loss: 6866314.630508475, MAPE: 0.14677844941616058\n",
      "Starting epoch 676\n",
      "Epoch 676 - Training loss: 6820210.766101695, MAPE: 0.14659827947616577\n",
      "Starting epoch 677\n",
      "Epoch 677 - Training loss: 6370484.176271186, MAPE: 0.14665067195892334\n",
      "Starting epoch 678\n",
      "Epoch 678 - Training loss: 5941792.759322034, MAPE: 0.14691103994846344\n",
      "Starting epoch 679\n",
      "Epoch 679 - Training loss: 6299651.416949153, MAPE: 0.1471037119626999\n",
      "Starting epoch 680\n",
      "Epoch 680 - Training loss: 6472124.311864407, MAPE: 0.14727625250816345\n",
      "Starting epoch 681\n",
      "Epoch 681 - Training loss: 5920455.918644068, MAPE: 0.14770643413066864\n",
      "Starting epoch 682\n",
      "Epoch 682 - Training loss: 7138160.488135593, MAPE: 0.1478331983089447\n",
      "Starting epoch 683\n",
      "Epoch 683 - Training loss: 6948797.288135593, MAPE: 0.14789101481437683\n",
      "Starting epoch 684\n",
      "Epoch 684 - Training loss: 6441124.555932203, MAPE: 0.14818599820137024\n",
      "Starting epoch 685\n",
      "Epoch 685 - Training loss: 9166510.210169492, MAPE: 0.14833082258701324\n",
      "Starting epoch 686\n",
      "Epoch 686 - Training loss: 6689302.128813559, MAPE: 0.14838673174381256\n",
      "Starting epoch 687\n",
      "Epoch 687 - Training loss: 7395473.898305085, MAPE: 0.14839890599250793\n",
      "Starting epoch 688\n",
      "Epoch 688 - Training loss: 8841661.722033897, MAPE: 0.14811931550502777\n",
      "Starting epoch 689\n",
      "Epoch 689 - Training loss: 6489940.93559322, MAPE: 0.14742739498615265\n",
      "Starting epoch 690\n",
      "Epoch 690 - Training loss: 6514284.366101695, MAPE: 0.14659646153450012\n",
      "Starting epoch 691\n",
      "Epoch 691 - Training loss: 6320190.698305084, MAPE: 0.14566397666931152\n",
      "Starting epoch 692\n",
      "Epoch 692 - Training loss: 5993659.715254237, MAPE: 0.14498694241046906\n",
      "Starting epoch 693\n",
      "Epoch 693 - Training loss: 6239116.691525424, MAPE: 0.1448298543691635\n",
      "Starting epoch 694\n",
      "Epoch 694 - Training loss: 9435180.583050847, MAPE: 0.1449510008096695\n",
      "Starting epoch 695\n",
      "Epoch 695 - Training loss: 5671729.030508474, MAPE: 0.14501747488975525\n",
      "Starting epoch 696\n",
      "Epoch 696 - Training loss: 6632804.447457627, MAPE: 0.14501380920410156\n",
      "Starting epoch 697\n",
      "Epoch 697 - Training loss: 6410166.671186441, MAPE: 0.14523927867412567\n",
      "Starting epoch 698\n",
      "Epoch 698 - Training loss: 6589100.474576271, MAPE: 0.14541882276535034\n",
      "Starting epoch 699\n",
      "Epoch 699 - Training loss: 7038294.020338983, MAPE: 0.1454218178987503\n",
      "Starting epoch 700\n",
      "Epoch 700 - Training loss: 7322055.376271186, MAPE: 0.14558131992816925\n",
      "Starting epoch 701\n",
      "Epoch 701 - Training loss: 5850454.562711865, MAPE: 0.14590372145175934\n",
      "Starting epoch 702\n",
      "Epoch 702 - Training loss: 8924006.291525424, MAPE: 0.14614520967006683\n",
      "Starting epoch 703\n",
      "Epoch 703 - Training loss: 6848073.545762712, MAPE: 0.14640794694423676\n",
      "Starting epoch 704\n",
      "Epoch 704 - Training loss: 8344291.905084746, MAPE: 0.14672556519508362\n",
      "Starting epoch 705\n",
      "Epoch 705 - Training loss: 5931894.183050848, MAPE: 0.14705069363117218\n",
      "Starting epoch 706\n",
      "Epoch 706 - Training loss: 5582110.752542373, MAPE: 0.1472819298505783\n",
      "Starting epoch 707\n",
      "Epoch 707 - Training loss: 6146374.074576271, MAPE: 0.14726193249225616\n",
      "Starting epoch 708\n",
      "Epoch 708 - Training loss: 8478069.477966102, MAPE: 0.1473996639251709\n",
      "Starting epoch 709\n",
      "Epoch 709 - Training loss: 6037719.430508475, MAPE: 0.14755293726921082\n",
      "Starting epoch 710\n",
      "Epoch 710 - Training loss: 7159941.749152542, MAPE: 0.14739541709423065\n",
      "Starting epoch 711\n",
      "Epoch 711 - Training loss: 9675323.769491525, MAPE: 0.14705991744995117\n",
      "Starting epoch 712\n",
      "Epoch 712 - Training loss: 9047371.38983051, MAPE: 0.1468823254108429\n",
      "Starting epoch 713\n",
      "Epoch 713 - Training loss: 8687506.657627119, MAPE: 0.14677034318447113\n",
      "Starting epoch 714\n",
      "Epoch 714 - Training loss: 6781700.013559322, MAPE: 0.14682026207447052\n",
      "Starting epoch 715\n",
      "Epoch 715 - Training loss: 8582810.46779661, MAPE: 0.14691302180290222\n",
      "Starting epoch 716\n",
      "Epoch 716 - Training loss: 6181359.837288136, MAPE: 0.14665649831295013\n",
      "Starting epoch 717\n",
      "Epoch 717 - Training loss: 7322428.311864407, MAPE: 0.14674560725688934\n",
      "Starting epoch 718\n",
      "Epoch 718 - Training loss: 5653601.138983051, MAPE: 0.14700421690940857\n",
      "Starting epoch 719\n",
      "Epoch 719 - Training loss: 6794392.298305085, MAPE: 0.14700429141521454\n",
      "Starting epoch 720\n",
      "Epoch 720 - Training loss: 7927670.454237288, MAPE: 0.14709804952144623\n",
      "Starting epoch 721\n",
      "Epoch 721 - Training loss: 5829849.274576271, MAPE: 0.1472974419593811\n",
      "Starting epoch 722\n",
      "Epoch 722 - Training loss: 6280929.518644067, MAPE: 0.1472981870174408\n",
      "Starting epoch 723\n",
      "Epoch 723 - Training loss: 5603674.630508475, MAPE: 0.147147998213768\n",
      "Starting epoch 724\n",
      "Epoch 724 - Training loss: 7005008.379661017, MAPE: 0.14694303274154663\n",
      "Starting epoch 725\n",
      "Epoch 725 - Training loss: 7642302.26440678, MAPE: 0.1471598893404007\n",
      "Starting epoch 726\n",
      "Epoch 726 - Training loss: 8344886.020338983, MAPE: 0.14774785935878754\n",
      "Starting epoch 727\n",
      "Epoch 727 - Training loss: 5959966.372881356, MAPE: 0.1480967402458191\n",
      "Starting epoch 728\n",
      "Epoch 728 - Training loss: 8236947.525423729, MAPE: 0.14830364286899567\n",
      "Starting epoch 729\n",
      "Epoch 729 - Training loss: 6558236.420338983, MAPE: 0.14849326014518738\n",
      "Starting epoch 730\n",
      "Epoch 730 - Training loss: 6807674.576271187, MAPE: 0.14862802624702454\n",
      "Starting epoch 731\n",
      "Epoch 731 - Training loss: 6396926.806779661, MAPE: 0.14876589179039001\n",
      "Starting epoch 732\n",
      "Epoch 732 - Training loss: 6831614.698305084, MAPE: 0.1486092060804367\n",
      "Starting epoch 733\n",
      "Epoch 733 - Training loss: 6277984.650847457, MAPE: 0.14820928871631622\n",
      "Starting epoch 734\n",
      "Epoch 734 - Training loss: 7621722.142372881, MAPE: 0.14803992211818695\n",
      "Starting epoch 735\n",
      "Epoch 735 - Training loss: 7372969.871186441, MAPE: 0.14794981479644775\n",
      "Starting epoch 736\n",
      "Epoch 736 - Training loss: 7092705.247457627, MAPE: 0.147755429148674\n",
      "Starting epoch 737\n",
      "Epoch 737 - Training loss: 5648671.837288136, MAPE: 0.14781278371810913\n",
      "Starting epoch 738\n",
      "Epoch 738 - Training loss: 5439151.538983051, MAPE: 0.14819729328155518\n",
      "Starting epoch 739\n",
      "Epoch 739 - Training loss: 9247386.142372882, MAPE: 0.14841699600219727\n",
      "Starting epoch 740\n",
      "Epoch 740 - Training loss: 6087178.738983051, MAPE: 0.1482982635498047\n",
      "Starting epoch 741\n",
      "Epoch 741 - Training loss: 6319601.030508474, MAPE: 0.14808985590934753\n",
      "Starting epoch 742\n",
      "Epoch 742 - Training loss: 8026039.972881356, MAPE: 0.148283913731575\n",
      "Starting epoch 743\n",
      "Epoch 743 - Training loss: 8725314.711864406, MAPE: 0.14871472120285034\n",
      "Starting epoch 744\n",
      "Epoch 744 - Training loss: 6123378.766101695, MAPE: 0.14904047548770905\n",
      "Starting epoch 745\n",
      "Epoch 745 - Training loss: 7129329.898305085, MAPE: 0.1488049030303955\n",
      "Starting epoch 746\n",
      "Epoch 746 - Training loss: 5918007.376271186, MAPE: 0.14824114739894867\n",
      "Starting epoch 747\n",
      "Epoch 747 - Training loss: 6899513.491525424, MAPE: 0.14773564040660858\n",
      "Starting epoch 748\n",
      "Epoch 748 - Training loss: 6995279.40338983, MAPE: 0.14721627533435822\n",
      "Starting epoch 749\n",
      "Epoch 749 - Training loss: 6002444.257627118, MAPE: 0.14696577191352844\n",
      "Starting epoch 750\n",
      "Epoch 750 - Training loss: 5684846.047457627, MAPE: 0.14711853861808777\n",
      "Starting epoch 751\n",
      "Epoch 751 - Training loss: 6506638.101694915, MAPE: 0.1470039039850235\n",
      "Starting epoch 752\n",
      "Epoch 752 - Training loss: 7944946.549152543, MAPE: 0.14676576852798462\n",
      "Starting epoch 753\n",
      "Epoch 753 - Training loss: 6154031.0779661015, MAPE: 0.14688755571842194\n",
      "Starting epoch 754\n",
      "Epoch 754 - Training loss: 6290784.86779661, MAPE: 0.1473114937543869\n",
      "Starting epoch 755\n",
      "Epoch 755 - Training loss: 5560303.457627119, MAPE: 0.14770907163619995\n",
      "Starting epoch 756\n",
      "Epoch 756 - Training loss: 6286914.061016949, MAPE: 0.1478203535079956\n",
      "Starting epoch 757\n",
      "Epoch 757 - Training loss: 6026134.237288136, MAPE: 0.14756877720355988\n",
      "Starting epoch 758\n",
      "Epoch 758 - Training loss: 5725000.081355932, MAPE: 0.1469419151544571\n",
      "Starting epoch 759\n",
      "Epoch 759 - Training loss: 5496784.59661017, MAPE: 0.14627471566200256\n",
      "Starting epoch 760\n",
      "Epoch 760 - Training loss: 6065681.898305085, MAPE: 0.14586573839187622\n",
      "Starting epoch 761\n",
      "Epoch 761 - Training loss: 10662038.779661017, MAPE: 0.14554817974567413\n",
      "Starting epoch 762\n",
      "Epoch 762 - Training loss: 10609802.847457627, MAPE: 0.14510893821716309\n",
      "Starting epoch 763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 763 - Training loss: 5880608.759322034, MAPE: 0.14462751150131226\n",
      "Starting epoch 764\n",
      "Epoch 764 - Training loss: 6597090.277966102, MAPE: 0.14430975914001465\n",
      "Starting epoch 765\n",
      "Epoch 765 - Training loss: 6630865.138983051, MAPE: 0.1441287249326706\n",
      "Starting epoch 766\n",
      "Epoch 766 - Training loss: 5559477.206779661, MAPE: 0.144115149974823\n",
      "Starting epoch 767\n",
      "Epoch 767 - Training loss: 5968983.430508475, MAPE: 0.14428187906742096\n",
      "Starting epoch 768\n",
      "Epoch 768 - Training loss: 10620549.857627118, MAPE: 0.14440853893756866\n",
      "Starting epoch 769\n",
      "Epoch 769 - Training loss: 5863287.430508475, MAPE: 0.14487077295780182\n",
      "Starting epoch 770\n",
      "Epoch 770 - Training loss: 5577804.52881356, MAPE: 0.14568810164928436\n",
      "Starting epoch 771\n",
      "Epoch 771 - Training loss: 8100273.898305085, MAPE: 0.14661656320095062\n",
      "Starting epoch 772\n",
      "Epoch 772 - Training loss: 5408841.084745763, MAPE: 0.14727811515331268\n",
      "Starting epoch 773\n",
      "Epoch 773 - Training loss: 6022503.376271186, MAPE: 0.14744220674037933\n",
      "Starting epoch 774\n",
      "Epoch 774 - Training loss: 5677717.8033898305, MAPE: 0.14750519394874573\n",
      "Starting epoch 775\n",
      "Epoch 775 - Training loss: 5899504.488135593, MAPE: 0.14727501571178436\n",
      "Starting epoch 776\n",
      "Epoch 776 - Training loss: 5674121.003389831, MAPE: 0.14673775434494019\n",
      "Starting epoch 777\n",
      "Epoch 777 - Training loss: 7812392.569491525, MAPE: 0.14624017477035522\n",
      "Starting epoch 778\n",
      "Epoch 778 - Training loss: 8202476.040677967, MAPE: 0.1457604616880417\n",
      "Starting epoch 779\n",
      "Epoch 779 - Training loss: 7448506.142372881, MAPE: 0.14528945088386536\n",
      "Starting epoch 780\n",
      "Epoch 780 - Training loss: 6101716.284745763, MAPE: 0.14512480795383453\n",
      "Starting epoch 781\n",
      "Epoch 781 - Training loss: 5343904.9220338985, MAPE: 0.1450001448392868\n",
      "Starting epoch 782\n",
      "Epoch 782 - Training loss: 8847095.53898305, MAPE: 0.14477898180484772\n",
      "Starting epoch 783\n",
      "Epoch 783 - Training loss: 6172315.010169491, MAPE: 0.1447836309671402\n",
      "Starting epoch 784\n",
      "Epoch 784 - Training loss: 8294588.311864407, MAPE: 0.14531713724136353\n",
      "Starting epoch 785\n",
      "Epoch 785 - Training loss: 6165310.047457627, MAPE: 0.14627301692962646\n",
      "Starting epoch 786\n",
      "Epoch 786 - Training loss: 5907663.620338983, MAPE: 0.1471959352493286\n",
      "Starting epoch 787\n",
      "Epoch 787 - Training loss: 5605460.447457627, MAPE: 0.14751680195331573\n",
      "Starting epoch 788\n",
      "Epoch 788 - Training loss: 5910916.772881356, MAPE: 0.14727680385112762\n",
      "Starting epoch 789\n",
      "Epoch 789 - Training loss: 5459367.050847458, MAPE: 0.1472867876291275\n",
      "Starting epoch 790\n",
      "Epoch 790 - Training loss: 6870272.4338983055, MAPE: 0.14729450643062592\n",
      "Starting epoch 791\n",
      "Epoch 791 - Training loss: 5611174.4, MAPE: 0.14698292315006256\n",
      "Starting epoch 792\n",
      "Epoch 792 - Training loss: 6034579.742372882, MAPE: 0.14692135155200958\n",
      "Starting epoch 793\n",
      "Epoch 793 - Training loss: 7869791.674576271, MAPE: 0.14707247912883759\n",
      "Starting epoch 794\n",
      "Epoch 794 - Training loss: 10636097.952542372, MAPE: 0.14711660146713257\n",
      "Starting epoch 795\n",
      "Epoch 795 - Training loss: 5584772.013559322, MAPE: 0.1472592055797577\n",
      "Starting epoch 796\n",
      "Epoch 796 - Training loss: 5538105.220338983, MAPE: 0.14769773185253143\n",
      "Starting epoch 797\n",
      "Epoch 797 - Training loss: 6568945.898305085, MAPE: 0.14774803817272186\n",
      "Starting epoch 798\n",
      "Epoch 798 - Training loss: 7764839.050847458, MAPE: 0.14748646318912506\n",
      "Starting epoch 799\n",
      "Epoch 799 - Training loss: 7735004.094915254, MAPE: 0.14694032073020935\n",
      "Starting epoch 800\n",
      "Epoch 800 - Training loss: 5441546.250847458, MAPE: 0.14636096358299255\n",
      "Starting epoch 801\n",
      "Epoch 801 - Training loss: 6875248.379661017, MAPE: 0.14612428843975067\n",
      "Starting epoch 802\n",
      "Epoch 802 - Training loss: 5566108.420338983, MAPE: 0.14586271345615387\n",
      "Starting epoch 803\n",
      "Epoch 803 - Training loss: 6157210.901694915, MAPE: 0.1456703096628189\n",
      "Starting epoch 804\n",
      "Epoch 804 - Training loss: 5799754.630508475, MAPE: 0.14540202915668488\n",
      "Starting epoch 805\n",
      "Epoch 805 - Training loss: 5703926.888135593, MAPE: 0.14514115452766418\n",
      "Starting epoch 806\n",
      "Epoch 806 - Training loss: 6861064.2440677965, MAPE: 0.14500798285007477\n",
      "Starting epoch 807\n",
      "Epoch 807 - Training loss: 5997751.322033898, MAPE: 0.14529365301132202\n",
      "Starting epoch 808\n",
      "Epoch 808 - Training loss: 6383353.057627118, MAPE: 0.1458556056022644\n",
      "Starting epoch 809\n",
      "Epoch 809 - Training loss: 8141017.816949152, MAPE: 0.14602147042751312\n",
      "Starting epoch 810\n",
      "Epoch 810 - Training loss: 9593441.19322034, MAPE: 0.14591087400913239\n",
      "Starting epoch 811\n",
      "Epoch 811 - Training loss: 5741723.444067797, MAPE: 0.14572304487228394\n",
      "Starting epoch 812\n",
      "Epoch 812 - Training loss: 5860839.484745762, MAPE: 0.14535439014434814\n",
      "Starting epoch 813\n",
      "Epoch 813 - Training loss: 6662187.06440678, MAPE: 0.14490504562854767\n",
      "Starting epoch 814\n",
      "Epoch 814 - Training loss: 5290492.501694915, MAPE: 0.14441294968128204\n",
      "Starting epoch 815\n",
      "Epoch 815 - Training loss: 5318136.108474576, MAPE: 0.14389877021312714\n",
      "Starting epoch 816\n",
      "Epoch 816 - Training loss: 5291544.298305085, MAPE: 0.14355982840061188\n",
      "Starting epoch 817\n",
      "Epoch 817 - Training loss: 5626909.179661017, MAPE: 0.14338219165802002\n",
      "Starting epoch 818\n",
      "Epoch 818 - Training loss: 5755560.569491525, MAPE: 0.14344173669815063\n",
      "Starting epoch 819\n",
      "Epoch 819 - Training loss: 6951124.610169492, MAPE: 0.14390678703784943\n",
      "Starting epoch 820\n",
      "Epoch 820 - Training loss: 7690780.420338983, MAPE: 0.14479206502437592\n",
      "Starting epoch 821\n",
      "Epoch 821 - Training loss: 5811051.389830508, MAPE: 0.14550304412841797\n",
      "Starting epoch 822\n",
      "Epoch 822 - Training loss: 7814877.39661017, MAPE: 0.14591944217681885\n",
      "Starting epoch 823\n",
      "Epoch 823 - Training loss: 5857922.60338983, MAPE: 0.1459478884935379\n",
      "Starting epoch 824\n",
      "Epoch 824 - Training loss: 5268209.979661017, MAPE: 0.14570623636245728\n",
      "Starting epoch 825\n",
      "Epoch 825 - Training loss: 5666704.054237288, MAPE: 0.1457691192626953\n",
      "Starting epoch 826\n",
      "Epoch 826 - Training loss: 7544015.186440678, MAPE: 0.1459384262561798\n",
      "Starting epoch 827\n",
      "Epoch 827 - Training loss: 5603949.722033898, MAPE: 0.1458195000886917\n",
      "Starting epoch 828\n",
      "Epoch 828 - Training loss: 7230745.057627118, MAPE: 0.14540040493011475\n",
      "Starting epoch 829\n",
      "Epoch 829 - Training loss: 5768606.372881356, MAPE: 0.14479473233222961\n",
      "Starting epoch 830\n",
      "Epoch 830 - Training loss: 5504459.281355932, MAPE: 0.144414484500885\n",
      "Starting epoch 831\n",
      "Epoch 831 - Training loss: 5385723.552542373, MAPE: 0.14455115795135498\n",
      "Starting epoch 832\n",
      "Epoch 832 - Training loss: 5407371.281355932, MAPE: 0.145192950963974\n",
      "Starting epoch 833\n",
      "Epoch 833 - Training loss: 5294869.640677966, MAPE: 0.14604036509990692\n",
      "Starting epoch 834\n",
      "Epoch 834 - Training loss: 8297363.742372882, MAPE: 0.14635947346687317\n",
      "Starting epoch 835\n",
      "Epoch 835 - Training loss: 6238438.616949152, MAPE: 0.14686226844787598\n",
      "Starting epoch 836\n",
      "Epoch 836 - Training loss: 5437531.715254237, MAPE: 0.1475526988506317\n",
      "Starting epoch 837\n",
      "Epoch 837 - Training loss: 5394704.813559322, MAPE: 0.14799733459949493\n",
      "Starting epoch 838\n",
      "Epoch 838 - Training loss: 6398362.6847457625, MAPE: 0.14825135469436646\n",
      "Starting epoch 839\n",
      "Epoch 839 - Training loss: 5773122.494915254, MAPE: 0.14806467294692993\n",
      "Starting epoch 840\n",
      "Epoch 840 - Training loss: 5367560.2440677965, MAPE: 0.14786042273044586\n",
      "Starting epoch 841\n",
      "Epoch 841 - Training loss: 6493018.250847458, MAPE: 0.14775295555591583\n",
      "Starting epoch 842\n",
      "Epoch 842 - Training loss: 5645351.2677966105, MAPE: 0.14755332469940186\n",
      "Starting epoch 843\n",
      "Epoch 843 - Training loss: 8753525.80338983, MAPE: 0.1469694972038269\n",
      "Starting epoch 844\n",
      "Epoch 844 - Training loss: 5092323.552542373, MAPE: 0.14620822668075562\n",
      "Starting epoch 845\n",
      "Epoch 845 - Training loss: 7758550.345762712, MAPE: 0.14558471739292145\n",
      "Starting epoch 846\n",
      "Epoch 846 - Training loss: 5877807.72881356, MAPE: 0.14513804018497467\n",
      "Starting epoch 847\n",
      "Epoch 847 - Training loss: 5076274.46779661, MAPE: 0.14487719535827637\n",
      "Starting epoch 848\n",
      "Epoch 848 - Training loss: 5864056.515254238, MAPE: 0.144614115357399\n",
      "Starting epoch 849\n",
      "Epoch 849 - Training loss: 7539125.261016949, MAPE: 0.14421628415584564\n",
      "Starting epoch 850\n",
      "Epoch 850 - Training loss: 5778249.979661017, MAPE: 0.14413757622241974\n",
      "Starting epoch 851\n",
      "Epoch 851 - Training loss: 5738790.833898305, MAPE: 0.14446957409381866\n",
      "Starting epoch 852\n",
      "Epoch 852 - Training loss: 5193540.040677967, MAPE: 0.14525125920772552\n",
      "Starting epoch 853\n",
      "Epoch 853 - Training loss: 6180674.169491526, MAPE: 0.14566397666931152\n",
      "Starting epoch 854\n",
      "Epoch 854 - Training loss: 5232254.644067797, MAPE: 0.14568088948726654\n",
      "Starting epoch 855\n",
      "Epoch 855 - Training loss: 6790588.745762712, MAPE: 0.14557813107967377\n",
      "Starting epoch 856\n",
      "Epoch 856 - Training loss: 5474117.152542373, MAPE: 0.14552733302116394\n",
      "Starting epoch 857\n",
      "Epoch 857 - Training loss: 7060105.979661017, MAPE: 0.145555779337883\n",
      "Starting epoch 858\n",
      "Epoch 858 - Training loss: 5252737.898305085, MAPE: 0.1452309489250183\n",
      "Starting epoch 859\n",
      "Epoch 859 - Training loss: 8933062.345762711, MAPE: 0.14486552774906158\n",
      "Starting epoch 860\n",
      "Epoch 860 - Training loss: 6943451.335593221, MAPE: 0.14458592236042023\n",
      "Starting epoch 861\n",
      "Epoch 861 - Training loss: 6461867.715254237, MAPE: 0.144126296043396\n",
      "Starting epoch 862\n",
      "Epoch 862 - Training loss: 5503419.06440678, MAPE: 0.1438160538673401\n",
      "Starting epoch 863\n",
      "Epoch 863 - Training loss: 6944842.305084745, MAPE: 0.14362600445747375\n",
      "Starting epoch 864\n",
      "Epoch 864 - Training loss: 8785002.847457627, MAPE: 0.14377433061599731\n",
      "Starting epoch 865\n",
      "Epoch 865 - Training loss: 7117707.06440678, MAPE: 0.14418022334575653\n",
      "Starting epoch 866\n",
      "Epoch 866 - Training loss: 7252605.938983051, MAPE: 0.14468646049499512\n",
      "Starting epoch 867\n",
      "Epoch 867 - Training loss: 7628570.033898305, MAPE: 0.14498068392276764\n",
      "Starting epoch 868\n",
      "Epoch 868 - Training loss: 5026831.525423729, MAPE: 0.14560721814632416\n",
      "Starting epoch 869\n",
      "Epoch 869 - Training loss: 7811836.257627118, MAPE: 0.14644600450992584\n",
      "Starting epoch 870\n",
      "Epoch 870 - Training loss: 5307523.037288136, MAPE: 0.1470661163330078\n",
      "Starting epoch 871\n",
      "Epoch 871 - Training loss: 5609076.610169492, MAPE: 0.1473555564880371\n",
      "Starting epoch 872\n",
      "Epoch 872 - Training loss: 5279765.911864406, MAPE: 0.14751498401165009\n",
      "Starting epoch 873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 873 - Training loss: 5437962.250847458, MAPE: 0.14765694737434387\n",
      "Starting epoch 874\n",
      "Epoch 874 - Training loss: 5261077.8033898305, MAPE: 0.14753440022468567\n",
      "Starting epoch 875\n",
      "Epoch 875 - Training loss: 7533275.661016949, MAPE: 0.14739544689655304\n",
      "Starting epoch 876\n",
      "Epoch 876 - Training loss: 5308159.457627119, MAPE: 0.14690949022769928\n",
      "Starting epoch 877\n",
      "Epoch 877 - Training loss: 5027807.376271186, MAPE: 0.1461242437362671\n",
      "Starting epoch 878\n",
      "Epoch 878 - Training loss: 5528653.66779661, MAPE: 0.1452571451663971\n",
      "Starting epoch 879\n",
      "Epoch 879 - Training loss: 7181416.569491525, MAPE: 0.14443667232990265\n",
      "Starting epoch 880\n",
      "Epoch 880 - Training loss: 5528344.840677966, MAPE: 0.14371807873249054\n",
      "Starting epoch 881\n",
      "Epoch 881 - Training loss: 5180592.325423729, MAPE: 0.14366887509822845\n",
      "Starting epoch 882\n",
      "Epoch 882 - Training loss: 7871580.962711864, MAPE: 0.14375826716423035\n",
      "Starting epoch 883\n",
      "Epoch 883 - Training loss: 5825593.057627118, MAPE: 0.14392085373401642\n",
      "Starting epoch 884\n",
      "Epoch 884 - Training loss: 7139121.681355932, MAPE: 0.14419977366924286\n",
      "Starting epoch 885\n",
      "Epoch 885 - Training loss: 9292822.020338982, MAPE: 0.14430172741413116\n",
      "Starting epoch 886\n",
      "Epoch 886 - Training loss: 5091463.40338983, MAPE: 0.14428649842739105\n",
      "Starting epoch 887\n",
      "Epoch 887 - Training loss: 6738328.949152542, MAPE: 0.14428389072418213\n",
      "Starting epoch 888\n",
      "Epoch 888 - Training loss: 5375943.213559322, MAPE: 0.14411747455596924\n",
      "Starting epoch 889\n",
      "Epoch 889 - Training loss: 5146796.52881356, MAPE: 0.14390116930007935\n",
      "Starting epoch 890\n",
      "Epoch 890 - Training loss: 5307314.061016949, MAPE: 0.14379601180553436\n",
      "Starting epoch 891\n",
      "Epoch 891 - Training loss: 5952564.393220339, MAPE: 0.14374923706054688\n",
      "Starting epoch 892\n",
      "Epoch 892 - Training loss: 5147607.213559322, MAPE: 0.14384129643440247\n",
      "Starting epoch 893\n",
      "Epoch 893 - Training loss: 5240846.644067797, MAPE: 0.14400063455104828\n",
      "Starting epoch 894\n",
      "Epoch 894 - Training loss: 6728844.908474577, MAPE: 0.14417614042758942\n",
      "Starting epoch 895\n",
      "Epoch 895 - Training loss: 8043479.105084746, MAPE: 0.14448487758636475\n",
      "Starting epoch 896\n",
      "Epoch 896 - Training loss: 5792068.338983051, MAPE: 0.14496447145938873\n",
      "Starting epoch 897\n",
      "Epoch 897 - Training loss: 6379188.06779661, MAPE: 0.1454852968454361\n",
      "Starting epoch 898\n",
      "Epoch 898 - Training loss: 5490572.908474577, MAPE: 0.14560826122760773\n",
      "Starting epoch 899\n",
      "Epoch 899 - Training loss: 7055186.223728813, MAPE: 0.14547328650951385\n",
      "Starting epoch 900\n",
      "Epoch 900 - Training loss: 5617881.816949152, MAPE: 0.145398810505867\n",
      "Starting epoch 901\n",
      "Epoch 901 - Training loss: 7643707.552542373, MAPE: 0.14530737698078156\n",
      "Starting epoch 902\n",
      "Epoch 902 - Training loss: 6082089.871186441, MAPE: 0.1449819952249527\n",
      "Starting epoch 903\n",
      "Epoch 903 - Training loss: 5373037.1254237285, MAPE: 0.14473426342010498\n",
      "Starting epoch 904\n",
      "Epoch 904 - Training loss: 6163446.671186441, MAPE: 0.14480659365653992\n",
      "Starting epoch 905\n",
      "Epoch 905 - Training loss: 6696917.911864406, MAPE: 0.14533297717571259\n",
      "Starting epoch 906\n",
      "Epoch 906 - Training loss: 5550228.610169492, MAPE: 0.1456330120563507\n",
      "Starting epoch 907\n",
      "Epoch 907 - Training loss: 4964334.942372882, MAPE: 0.1455121636390686\n",
      "Starting epoch 908\n",
      "Epoch 908 - Training loss: 8030023.701694915, MAPE: 0.14541484415531158\n",
      "Starting epoch 909\n",
      "Epoch 909 - Training loss: 4989255.945762712, MAPE: 0.14522764086723328\n",
      "Starting epoch 910\n",
      "Epoch 910 - Training loss: 5425696.325423729, MAPE: 0.14489546418190002\n",
      "Starting epoch 911\n",
      "Epoch 911 - Training loss: 8636220.311864406, MAPE: 0.14465786516666412\n",
      "Starting epoch 912\n",
      "Epoch 912 - Training loss: 5916492.366101695, MAPE: 0.14475372433662415\n",
      "Starting epoch 913\n",
      "Epoch 913 - Training loss: 6669630.698305084, MAPE: 0.1452813446521759\n",
      "Starting epoch 914\n",
      "Epoch 914 - Training loss: 7982930.440677966, MAPE: 0.14580653607845306\n",
      "Starting epoch 915\n",
      "Epoch 915 - Training loss: 5274270.698305084, MAPE: 0.1460699439048767\n",
      "Starting epoch 916\n",
      "Epoch 916 - Training loss: 5395477.8033898305, MAPE: 0.14604265987873077\n",
      "Starting epoch 917\n",
      "Epoch 917 - Training loss: 6813607.484745762, MAPE: 0.1458304524421692\n",
      "Starting epoch 918\n",
      "Epoch 918 - Training loss: 4894942.183050848, MAPE: 0.14611342549324036\n",
      "Starting epoch 919\n",
      "Epoch 919 - Training loss: 5332371.905084746, MAPE: 0.14660240709781647\n",
      "Starting epoch 920\n",
      "Epoch 920 - Training loss: 5225630.101694915, MAPE: 0.14664819836616516\n",
      "Starting epoch 921\n",
      "Epoch 921 - Training loss: 7606208.650847457, MAPE: 0.146479532122612\n",
      "Starting epoch 922\n",
      "Epoch 922 - Training loss: 5216983.701694915, MAPE: 0.1462278664112091\n",
      "Starting epoch 923\n",
      "Epoch 923 - Training loss: 9426506.847457627, MAPE: 0.14559021592140198\n",
      "Starting epoch 924\n",
      "Epoch 924 - Training loss: 6289312.0, MAPE: 0.14495395123958588\n",
      "Starting epoch 925\n",
      "Epoch 925 - Training loss: 5788895.13220339, MAPE: 0.14443521201610565\n",
      "Starting epoch 926\n",
      "Epoch 926 - Training loss: 6192862.589830508, MAPE: 0.143972247838974\n",
      "Starting epoch 927\n",
      "Epoch 927 - Training loss: 5923528.677966102, MAPE: 0.143622487783432\n",
      "Starting epoch 928\n",
      "Epoch 928 - Training loss: 5294115.145762712, MAPE: 0.1435069441795349\n",
      "Starting epoch 929\n",
      "Epoch 929 - Training loss: 5246507.769491525, MAPE: 0.14358025789260864\n",
      "Starting epoch 930\n",
      "Epoch 930 - Training loss: 5467486.589830508, MAPE: 0.14376094937324524\n",
      "Starting epoch 931\n",
      "Epoch 931 - Training loss: 8031498.738983051, MAPE: 0.14405621588230133\n",
      "Starting epoch 932\n",
      "Epoch 932 - Training loss: 7010595.3627118645, MAPE: 0.1446271687746048\n",
      "Starting epoch 933\n",
      "Epoch 933 - Training loss: 6484743.593220339, MAPE: 0.14538773894309998\n",
      "Starting epoch 934\n",
      "Epoch 934 - Training loss: 6747578.847457627, MAPE: 0.14639104902744293\n",
      "Starting epoch 935\n",
      "Epoch 935 - Training loss: 4946535.457627119, MAPE: 0.14734263718128204\n",
      "Starting epoch 936\n",
      "Epoch 936 - Training loss: 7690578.006779661, MAPE: 0.14786146581172943\n",
      "Starting epoch 937\n",
      "Epoch 937 - Training loss: 7544256.0, MAPE: 0.14777524769306183\n",
      "Starting epoch 938\n",
      "Epoch 938 - Training loss: 4937461.098305085, MAPE: 0.14749419689178467\n",
      "Starting epoch 939\n",
      "Epoch 939 - Training loss: 5494994.223728813, MAPE: 0.1473408043384552\n",
      "Starting epoch 940\n",
      "Epoch 940 - Training loss: 5085458.494915254, MAPE: 0.14716273546218872\n",
      "Starting epoch 941\n",
      "Epoch 941 - Training loss: 7714857.871186441, MAPE: 0.14670810103416443\n",
      "Starting epoch 942\n",
      "Epoch 942 - Training loss: 5751892.284745763, MAPE: 0.14645127952098846\n",
      "Starting epoch 943\n",
      "Epoch 943 - Training loss: 5520261.857627119, MAPE: 0.14647585153579712\n",
      "Starting epoch 944\n",
      "Epoch 944 - Training loss: 6369485.450847457, MAPE: 0.1462731510400772\n",
      "Starting epoch 945\n",
      "Epoch 945 - Training loss: 8049741.993220339, MAPE: 0.14647071063518524\n",
      "Starting epoch 946\n",
      "Epoch 946 - Training loss: 5303232.0, MAPE: 0.1467438042163849\n",
      "Starting epoch 947\n",
      "Epoch 947 - Training loss: 6731704.189830508, MAPE: 0.14622196555137634\n",
      "Starting epoch 948\n",
      "Epoch 948 - Training loss: 5147389.39661017, MAPE: 0.14545997977256775\n",
      "Starting epoch 949\n",
      "Epoch 949 - Training loss: 5826228.93559322, MAPE: 0.14466418325901031\n",
      "Starting epoch 950\n",
      "Epoch 950 - Training loss: 5504552.135593221, MAPE: 0.14430111646652222\n",
      "Starting epoch 951\n",
      "Epoch 951 - Training loss: 5123920.705084746, MAPE: 0.1442107856273651\n",
      "Starting epoch 952\n",
      "Epoch 952 - Training loss: 5427789.016949153, MAPE: 0.14399680495262146\n",
      "Starting epoch 953\n",
      "Epoch 953 - Training loss: 5792491.93220339, MAPE: 0.14365242421627045\n",
      "Starting epoch 954\n",
      "Epoch 954 - Training loss: 5280836.230508475, MAPE: 0.14350134134292603\n",
      "Starting epoch 955\n",
      "Epoch 955 - Training loss: 5593115.227118644, MAPE: 0.1436062753200531\n",
      "Starting epoch 956\n",
      "Epoch 956 - Training loss: 7507068.854237288, MAPE: 0.1436997652053833\n",
      "Starting epoch 957\n",
      "Epoch 957 - Training loss: 5527241.871186441, MAPE: 0.14389678835868835\n",
      "Starting epoch 958\n",
      "Epoch 958 - Training loss: 5174329.925423729, MAPE: 0.14411984384059906\n",
      "Starting epoch 959\n",
      "Epoch 959 - Training loss: 5019622.671186441, MAPE: 0.14433899521827698\n",
      "Starting epoch 960\n",
      "Epoch 960 - Training loss: 5347928.081355932, MAPE: 0.1444288194179535\n",
      "Starting epoch 961\n",
      "Epoch 961 - Training loss: 4936103.972881356, MAPE: 0.14445647597312927\n",
      "Starting epoch 962\n",
      "Epoch 962 - Training loss: 4931469.016949153, MAPE: 0.14448383450508118\n",
      "Starting epoch 963\n",
      "Epoch 963 - Training loss: 5228768.976271186, MAPE: 0.14420495927333832\n",
      "Starting epoch 964\n",
      "Epoch 964 - Training loss: 5769946.6847457625, MAPE: 0.143906369805336\n",
      "Starting epoch 965\n",
      "Epoch 965 - Training loss: 6610391.538983051, MAPE: 0.14376139640808105\n",
      "Starting epoch 966\n",
      "Epoch 966 - Training loss: 6220101.749152542, MAPE: 0.1435638666152954\n",
      "Starting epoch 967\n",
      "Epoch 967 - Training loss: 6104957.830508474, MAPE: 0.1433285027742386\n",
      "Starting epoch 968\n",
      "Epoch 968 - Training loss: 4993898.793220339, MAPE: 0.14309613406658173\n",
      "Starting epoch 969\n",
      "Epoch 969 - Training loss: 4984627.145762712, MAPE: 0.1427936553955078\n",
      "Starting epoch 970\n",
      "Epoch 970 - Training loss: 5549109.261016949, MAPE: 0.14253799617290497\n",
      "Starting epoch 971\n",
      "Epoch 971 - Training loss: 5994827.281355932, MAPE: 0.1424523890018463\n",
      "Starting epoch 972\n",
      "Epoch 972 - Training loss: 7466318.210169491, MAPE: 0.1425926685333252\n",
      "Starting epoch 973\n",
      "Epoch 973 - Training loss: 5999472.813559322, MAPE: 0.14272820949554443\n",
      "Starting epoch 974\n",
      "Epoch 974 - Training loss: 8105669.098305085, MAPE: 0.14269603788852692\n",
      "Starting epoch 975\n",
      "Epoch 975 - Training loss: 5663775.674576271, MAPE: 0.14272795617580414\n",
      "Starting epoch 976\n",
      "Epoch 976 - Training loss: 5312926.26440678, MAPE: 0.14305588603019714\n",
      "Starting epoch 977\n",
      "Epoch 977 - Training loss: 5373228.040677967, MAPE: 0.14343880116939545\n",
      "Starting epoch 978\n",
      "Epoch 978 - Training loss: 5337198.644067797, MAPE: 0.14383549988269806\n",
      "Starting epoch 979\n",
      "Epoch 979 - Training loss: 5100998.128813559, MAPE: 0.1443406343460083\n",
      "Starting epoch 980\n",
      "Epoch 980 - Training loss: 5253321.654237288, MAPE: 0.14456798136234283\n",
      "Starting epoch 981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 981 - Training loss: 5436402.494915254, MAPE: 0.14472328126430511\n",
      "Starting epoch 982\n",
      "Epoch 982 - Training loss: 5922047.023728814, MAPE: 0.14492163062095642\n",
      "Starting epoch 983\n",
      "Epoch 983 - Training loss: 5019228.583050847, MAPE: 0.14496950805187225\n",
      "Starting epoch 984\n",
      "Epoch 984 - Training loss: 5112595.688135593, MAPE: 0.14481301605701447\n",
      "Starting epoch 985\n",
      "Epoch 985 - Training loss: 6120952.949152542, MAPE: 0.14479714632034302\n",
      "Starting epoch 986\n",
      "Epoch 986 - Training loss: 6353849.274576271, MAPE: 0.14522314071655273\n",
      "Starting epoch 987\n",
      "Epoch 987 - Training loss: 5420870.183050848, MAPE: 0.14537326991558075\n",
      "Starting epoch 988\n",
      "Epoch 988 - Training loss: 7006539.1728813555, MAPE: 0.1457194685935974\n",
      "Starting epoch 989\n",
      "Epoch 989 - Training loss: 6278316.366101695, MAPE: 0.14619265496730804\n",
      "Starting epoch 990\n",
      "Epoch 990 - Training loss: 6584812.908474577, MAPE: 0.14618143439292908\n",
      "Starting epoch 991\n",
      "Epoch 991 - Training loss: 6203966.806779661, MAPE: 0.14585666358470917\n",
      "Starting epoch 992\n",
      "Epoch 992 - Training loss: 4920499.145762712, MAPE: 0.14550527930259705\n",
      "Starting epoch 993\n",
      "Epoch 993 - Training loss: 9119797.477966102, MAPE: 0.1448216289281845\n",
      "Starting epoch 994\n",
      "Epoch 994 - Training loss: 6725242.901694915, MAPE: 0.1446039378643036\n",
      "Starting epoch 995\n",
      "Epoch 995 - Training loss: 4796496.840677966, MAPE: 0.14449788630008698\n",
      "Starting epoch 996\n",
      "Epoch 996 - Training loss: 5073677.722033898, MAPE: 0.14411507546901703\n",
      "Starting epoch 997\n",
      "Epoch 997 - Training loss: 7123878.616949152, MAPE: 0.14384636282920837\n",
      "Starting epoch 998\n",
      "Epoch 998 - Training loss: 5363414.237288136, MAPE: 0.1435721516609192\n",
      "Starting epoch 999\n",
      "Epoch 999 - Training loss: 5207821.993220339, MAPE: 0.14342033863067627\n",
      "Starting epoch 1000\n",
      "Epoch 1000 - Training loss: 4955083.06440678, MAPE: 0.14326129853725433\n",
      "Training process has finished.\n"
     ]
    }
   ],
   "source": [
    "import torchmetrics\n",
    "#loss_values = []\n",
    "best_MSE = float(\"inf\")\n",
    "best_MAPE = 2\n",
    "best_MAE = float(\"inf\")\n",
    "for epoch in range(0, 1000): \n",
    "    print(f'Starting epoch {epoch+1}')\n",
    "    metric = torchmetrics.MeanAbsolutePercentageError()\n",
    "    metric_MAE = torchmetrics.MeanAbsoluteError()\n",
    "    current_loss = 0.0\n",
    "    for data, labels in train_loader:\n",
    "        labels = labels.unsqueeze(1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        loss = loss_function(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        current_loss += loss.item()\n",
    "        metric.update(output, labels)\n",
    "        metric_MAE.update(output, labels)\n",
    "    else:\n",
    "        mape = metric.compute().item()\n",
    "        print(\"Epoch {} - Training loss: {}, MAPE: {}\".format(epoch+1, current_loss/len(train_dataset), mape))\n",
    "        best_MSE = min(best_MSE, current_loss/len(train_dataset))\n",
    "        best_MAE = min(best_MAE, metric_MAE.compute().item())\n",
    "        best_MAPE = min(best_MAPE, mape)\n",
    "    #loss_values.append(current_loss / len(train_dataset))\n",
    "\n",
    "print('Training process has finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9dfffd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def show_graphics(y_test, y_pred):\n",
    "    print(\"\\n\\nGraphic: \")\n",
    "    x_ax = range(len(y_test))\n",
    "    plt.plot(x_ax, y_test, label='original')\n",
    "    plt.plot(x_ax, y_pred, label='predicted')\n",
    "    plt.title(\"Laptop prices\")\n",
    "    plt.xlabel(\"X-axis\")\n",
    "    plt.ylabel(\"Y-axis\")\n",
    "    plt.legend(loc='best', fancybox=True, shadow=True)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c170031f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = torch.Tensor()\n",
    "target = torch.Tensor()\n",
    "MAE = torchmetrics.MeanAbsoluteError()\n",
    "MSE = torchmetrics.MeanSquaredError()\n",
    "MAPE = torchmetrics.MeanAbsolutePercentageError()\n",
    "with torch.no_grad():\n",
    "    for data,labels in test_loader:\n",
    "        output=model(data) \n",
    "        predicted = torch.cat((predicted, output))\n",
    "        target = torch.cat((target, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "52114dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  10420.0068359375 Train MAE 12163.568359375\n",
      "MSE:  357321952.0 Train MSE 4796496.840677966\n",
      "MAPE:  0.13262735307216644 Train MAPE 0.1424523890018463\n"
     ]
    }
   ],
   "source": [
    "target = target.unsqueeze(1)\n",
    "print(\"MAE: \", MAE(predicted, target).item(), \"Train MAE\", best_MAE)\n",
    "print(\"MSE: \", MSE(predicted, target).item(), \"Train MSE\", best_MSE)\n",
    "print(\"MAPE: \", MAPE(predicted, target).item(), \"Train MAPE\", best_MAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9f89cfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Graphic: \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_graphics(target.tolist(), predicted.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d184ab1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(16441.2461)\n",
      "tensor(13990.)\n",
      "tensor(283653.0625)\n",
      "tensor(309990.)\n",
      "tensor(74532.5547)\n",
      "tensor(76063.2500)\n"
     ]
    }
   ],
   "source": [
    "print(predicted.min())\n",
    "print(target.min())\n",
    "print(predicted.max())\n",
    "print(target.max())\n",
    "print(predicted.mean())\n",
    "print(target.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9b2f4d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./model_reg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3490f9d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
